# per_sample_label_stability

Label stability is defined as the absolute difference between the number of times the sample is classified as 0 and 1. If the absolute difference is large, the label is more stable. If the difference is exactly zero, then it's extremely unstable --- equally likely to be classified as 0 or 1.



## Parameters

- **predicted_labels** (*list*)




