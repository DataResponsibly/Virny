{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Redirect to introduction/welcome_to_virny.md</p>"},{"location":"api/overview/","title":"Overview","text":""},{"location":"api/overview/#analyzers","title":"analyzers","text":"<p>Subgroup Error and Variance Analyzers.</p> <p>This module contains fairness and stability analysing methods for defined subgroups. The purpose of an analyzer is to analyse defined metrics for defined subgroups.</p> <ul> <li>AbstractOverallVarianceAnalyzer</li> <li>AbstractSubgroupAnalyzer</li> <li>BatchOverallVarianceAnalyzer</li> <li>BatchOverallVarianceAnalyzerPostProcessing</li> <li>SubgroupErrorAnalyzer</li> <li>SubgroupVarianceAnalyzer</li> <li>SubgroupVarianceCalculator</li> </ul>"},{"location":"api/overview/#configs","title":"configs","text":"<p>Configs amd constants for the source code logic.</p>"},{"location":"api/overview/#custom_classes","title":"custom_classes","text":"<p>This module contains custom classes for metrics computation interfaces. The purpose is to split metrics computation and visualization pipeline on components that are highly  customizable for future library features.</p> <ul> <li>BaseFlowDataset</li> <li>MetricsComposer</li> <li>MetricsInteractiveVisualizer</li> <li>MetricsVisualizer</li> </ul>"},{"location":"api/overview/#datasets","title":"datasets","text":"<p>This module contains sample datasets and data loaders. The purpose is to provide sample datasets for functionality testing and show examples of data loaders (aka dataset classes).</p> <ul> <li>ACSEmploymentDataset</li> <li>ACSIncomeDataset</li> <li>ACSMobilityDataset</li> <li>ACSPublicCoverageDataset</li> <li>ACSTravelTimeDataset</li> <li>BankMarketingDataset</li> <li>CardiovascularDiseaseDataset</li> <li>CompasDataset</li> <li>CompasWithoutSensitiveAttrsDataset</li> <li>DiabetesDataset2019</li> <li>GermanCreditDataset</li> <li>LawSchoolDataset</li> <li>RicciDataset</li> <li>StudentPerformancePortugueseDataset</li> </ul>"},{"location":"api/overview/#metrics","title":"metrics","text":"<p>This module contains functions for computing subgroup variance and error metrics.</p> <ul> <li>aleatoric_uncertainty</li> <li>confusion_matrix_metrics</li> <li>iqr</li> <li>jitter</li> <li>label_stability</li> <li>mean_prediction</li> <li>overall_uncertainty</li> <li>statistical_bias</li> <li>std</li> </ul>"},{"location":"api/overview/#preprocessing","title":"preprocessing","text":"<p>Preprocessing techniques.</p> <p>This module contains function for input dataset preprocessing.</p> <ul> <li>get_dummies</li> <li>make_features_dfs</li> <li>preprocess_dataset</li> </ul>"},{"location":"api/overview/#user_interfaces","title":"user_interfaces","text":"<p>User interfaces.</p> <p>This module contains user interfaces for metrics computation.</p> <ul> <li>compute_metrics_with_config</li> <li>compute_metrics_with_db_writer</li> <li>compute_metrics_with_multiple_test_sets</li> </ul>"},{"location":"api/overview/#utils","title":"utils","text":"<p>Common helpers and utils.</p> <ul> <li>count_prediction_metrics</li> <li>create_test_protected_groups</li> <li>tune_ML_models</li> <li>validate_config</li> </ul>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/","title":"AbstractOverallVarianceAnalyzer","text":"<p>Abstract class for an analyzer that computes overall variance metrics for subgroups.</p>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'HoeffdingTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> <li> <p>random_state (int) \u2013 defaults to <code>None</code></p> <p>[Optional] Controls the randomness of the bootstrap approach for model arbitrariness evaluation</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] A flag if model can return probabilities for its predictions.  If no, only metrics based on labels (not labels and probabilities) will be computed.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/AbstractOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Return a dictionary where keys are models indexes, and values are lists of  correspondent model predictions for X_test set.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> compute_metrics <p>Measure metrics for the base model. Save results to a .csv file.</p> <p>Parameters</p> <ul> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_metrics_to_file"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/","title":"AbstractSubgroupAnalyzer","text":"<p>Abstract class for a subgroup analyzer to compute metrics for subgroups.</p>"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>test_protected_groups (dict)</p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>A mode to compute metrics. It can have two values 'error_analysis' and default (None).</p> </li> </ul>"},{"location":"api/analyzers/AbstractSubgroupAnalyzer/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute metrics for each subgroup in self.test_protected_groups using _compute_metrics method.</p> <p>Return a dictionary where keys are subgroup names, and values are subgroup metrics.</p> <p>Parameters</p> <ul> <li>y_preds </li> <li>models_predictions     (dict)    </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/","title":"BatchOverallVarianceAnalyzer","text":"<p>Analyzer to compute subgroup variance metrics for batch learning models.</p>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'DecisionTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>target_column (str)</p> <p>Name of the target column</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> <li> <p>random_state (int) \u2013 defaults to <code>None</code></p> <p>[Optional] Controls the randomness of the bootstrap approach for model arbitrariness evaluation</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] A flag if model can return probabilities for its predictions.  If no, only metrics based on labels (not labels and probabilities) will be computed.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzer/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples.</p> <p>Return a dictionary where keys are models indexes, and values are lists of  correspondent model predictions for X_test set.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> compute_metrics <p>Measure metrics for the base model. Save results to a .csv file.</p> <p>Parameters</p> <ul> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_metrics_to_file"},{"location":"api/analyzers/BatchOverallVarianceAnalyzerPostProcessing/","title":"BatchOverallVarianceAnalyzerPostProcessing","text":"<p>Analyzer to compute subgroup variance metrics using the defined post-processor.</p>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzerPostProcessing/#parameters","title":"Parameters","text":"<ul> <li> <p>postprocessor</p> <p>One of postprocessors from aif360 (https://aif360.readthedocs.io/en/stable/modules/algorithms.html#module-aif360.algorithms.postprocessing)</p> </li> <li> <p>sensitive_attribute (str)</p> <p>A sensitive attribute to use for post-processing</p> </li> <li> <p>base_model</p> <p>Base model for stability measuring</p> </li> <li> <p>base_model_name (str)</p> <p>Model name like 'DecisionTreeClassifier' or 'LogisticRegression'</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>X_train (pandas.core.frame.DataFrame)</p> <p>Processed features train set</p> </li> <li> <p>y_train (pandas.core.frame.DataFrame)</p> <p>Targets train set</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>target_column (str)</p> <p>Name of the target column</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators in ensemble to measure base_model stability</p> </li> <li> <p>random_state (int) \u2013 defaults to <code>None</code></p> <p>[Optional] Controls the randomness of the bootstrap approach for model arbitrariness evaluation</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] A flag if model can return probabilities for its predictions.  If no, only metrics based on labels (not labels and probabilities) will be computed.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/BatchOverallVarianceAnalyzerPostProcessing/#methods","title":"Methods","text":"UQ_by_boostrap <p>Quantifying uncertainty of the base model by constructing an ensemble from bootstrapped samples and applying postprocessing intervention.</p> <p>Return a dictionary where keys are models indexes, and values are lists of  correspondent model predictions for X_test set.</p> <p>Parameters</p> <ul> <li>boostrap_size     (int)    </li> <li>with_replacement     (bool)    </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> compute_metrics <p>Measure metrics for the base model. Save results to a .csv file.</p> <p>Parameters</p> <ul> <li>save_results     (bool)     \u2013 defaults to <code>True</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> save_metrics_to_file"},{"location":"api/analyzers/SubgroupErrorAnalyzer/","title":"SubgroupErrorAnalyzer","text":"<p>Analyzer to compute error metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupErrorAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_protected_groups (dict) \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupErrorAnalyzer/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute metrics for each subgroup in self.test_protected_groups using _compute_metrics method.</p> <p>Return a dictionary where keys are subgroup names, and values are subgroup metrics.</p> <p>Parameters</p> <ul> <li>y_preds </li> <li>models_predictions     (dict)    </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/","title":"SubgroupVarianceAnalyzer","text":"<p>Analyzer to compute variance metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/#parameters","title":"Parameters","text":"<ul> <li> <p>model_setting (metrics.ModelSetting)</p> <p>Model learning type; a constant from virny.configs.constants.ModelSetting</p> </li> <li> <p>n_estimators (int)</p> <p>Number of estimators for bootstrap</p> </li> <li> <p>base_model</p> <p>Initialized base model to analyze</p> </li> <li> <p>base_model_name (str)</p> <p>Model name</p> </li> <li> <p>bootstrap_fraction (float)</p> <p>[0-1], fraction from train_pd_dataset for fitting an ensemble of base models</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>Initialized object of GenericPipeline class</p> </li> <li> <p>dataset_name (str)</p> <p>Name of dataset, used for correct results naming</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> <li> <p>test_protected_groups (dict)</p> <p>A dictionary of protected groups where keys are subgroup names,  and values are X_test row indexes correspondent to this subgroup.</p> </li> <li> <p>postprocessor \u2013 defaults to <code>None</code></p> <p>One of postprocessors from aif360 (https://aif360.readthedocs.io/en/stable/modules/algorithms.html#module-aif360.algorithms.postprocessing)</p> </li> <li> <p>postprocessing_sensitive_attribute (str) \u2013 defaults to <code>None</code></p> <p>A sensitive attribute to use for post-processing</p> </li> <li> <p>random_state (int) \u2013 defaults to <code>None</code></p> <p>[Optional] Controls the randomness of the bootstrap approach for model arbitrariness evaluation</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] True, if models in models_config have a predict_proba method and can return probabilities for predictions,  False, otherwise. Note that if it is set to False, only metrics based on labels (not labels and probabilities) will be computed.  Ignored when a postprocessor is not None, and set to False in this case.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.  As for now, 0, 1, 2 levels are supported.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceAnalyzer/#methods","title":"Methods","text":"compute_metrics <p>Measure variance metrics for subgroups for the base model. Display variance plots for analysis if needed.  Save results to a .csv file if needed.</p> <p>Return averaged bootstrap predictions and a pandas dataframe of variance metrics for subgroups.</p> <p>Parameters</p> <ul> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> <li>with_fit     (bool)     \u2013 defaults to <code>True</code> </li> </ul> set_test_protected_groups set_test_sets"},{"location":"api/analyzers/SubgroupVarianceCalculator/","title":"SubgroupVarianceCalculator","text":"<p>Calculator that calculates variance metrics for subgroups.</p>"},{"location":"api/analyzers/SubgroupVarianceCalculator/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Processed features test set</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Targets test set</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these subgroups</p> </li> <li> <p>test_protected_groups \u2013 defaults to <code>None</code></p> <p>A dictionary where keys are sensitive attributes, and values input dataset rows  that are correspondent to these sensitive attributes.</p> </li> <li> <p>computation_mode (str) \u2013 defaults to <code>None</code></p> <p>[Optional] A non-default mode for metrics computation. Should be included in the ComputationMode enum.</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] A flag if model can return probabilities for its predictions.  If no, only metrics based on labels (not labels and probabilities) will be computed.</p> </li> </ul>"},{"location":"api/analyzers/SubgroupVarianceCalculator/#methods","title":"Methods","text":"compute_subgroup_metrics <p>Compute variance metrics for subgroups.</p> <p>Return a dict of dicts where key is 'overall' or a subgroup name, and value is a dict of metrics for this subgroup.</p> <p>Parameters</p> <ul> <li>y_preds </li> <li>models_predictions     (dict)    </li> <li>save_results     (bool)    </li> <li>result_filename     (str)     \u2013 defaults to <code>None</code> </li> <li>save_dir_path     (str)     \u2013 defaults to <code>None</code> </li> </ul> save_metrics_to_file <p>Parameters</p> <ul> <li>result_filename     (str)    </li> <li>save_dir_path     (str)    </li> </ul> set_overall_variance_metrics"},{"location":"api/custom-classes/BaseFlowDataset/","title":"BaseFlowDataset","text":"<p>Dataset class with custom train and test splits that is used as input for metrics computation interfaces. Create your dataset class based on this one to use it for metrics computation interfaces.</p>"},{"location":"api/custom-classes/BaseFlowDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>init_sensitive_attrs_df (pandas.core.frame.DataFrame)</p> <p>Full train + test non-preprocessed dataset of sensitive attributes with initial indexes.  It is used for creating test groups.</p> </li> <li> <p>X_train_val (pandas.core.frame.DataFrame)</p> <p>Train dataframe of features</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Test dataframe of features</p> </li> <li> <p>y_train_val (pandas.core.frame.DataFrame)</p> <p>Train dataframe with a target column</p> </li> <li> <p>y_test (pandas.core.frame.DataFrame)</p> <p>Test dataframe with a target column</p> </li> <li> <p>target (str)</p> <p>Name of the target column name</p> </li> <li> <p>numerical_columns (list)</p> <p>List of numerical column names</p> </li> <li> <p>categorical_columns (list)</p> <p>List of categorical column names</p> </li> </ul>"},{"location":"api/custom-classes/MetricsComposer/","title":"MetricsComposer","text":"<p>Metric Composer class that combines different subgroup metrics to create disparity metrics  such as 'Disparate_Impact' or 'Accuracy_Difference'.</p> <p>Definitions of the disparity metrics could be observed in the init method of the Metric Composer:  https://github.com/DataResponsibly/Virny/blob/main/virny/custom_classes/metrics_composer.py</p>"},{"location":"api/custom-classes/MetricsComposer/#parameters","title":"Parameters","text":"<ul> <li> <p>models_metrics_dct (dict)</p> <p>Dictionary where keys are model names and values are dataframes of subgroups metrics for each model</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are privilege values for these attributes</p> </li> </ul>"},{"location":"api/custom-classes/MetricsComposer/#methods","title":"Methods","text":"compose_metrics <p>Compose subgroup metrics from self.model_metrics_df.</p> <p>Return a dictionary of composed metrics.</p>"},{"location":"api/custom-classes/MetricsInteractiveVisualizer/","title":"MetricsInteractiveVisualizer","text":"<p>Class to create an interactive web app based on models metrics.</p>"},{"location":"api/custom-classes/MetricsInteractiveVisualizer/#parameters","title":"Parameters","text":"<ul> <li> <p>X_data (pandas.core.frame.DataFrame)</p> <p>An original features dataframe</p> </li> <li> <p>y_data (pandas.core.frame.DataFrame)</p> <p>An original target column pandas series</p> </li> <li> <p>model_metrics</p> <p>A dictionary or a dataframe where keys are model names and values are dataframes of subgroup metrics for each model</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these attributes</p> </li> </ul>"},{"location":"api/custom-classes/MetricsInteractiveVisualizer/#methods","title":"Methods","text":"create_web_app <p>Build an interactive web application.</p> <p>Parameters</p> <ul> <li>start_app     \u2013 defaults to <code>True</code> </li> </ul>"},{"location":"api/custom-classes/MetricsVisualizer/","title":"MetricsVisualizer","text":"<p>Class to create useful visualizations of models metrics.</p>"},{"location":"api/custom-classes/MetricsVisualizer/#parameters","title":"Parameters","text":"<ul> <li> <p>models_metrics_dct (dict)</p> <p>Dictionary where keys are model names and values are dataframes of subgroup metrics for each model</p> </li> <li> <p>models_composed_metrics_df (pandas.core.frame.DataFrame)</p> <p>Dataframe of all model composed metrics</p> </li> <li> <p>dataset_name (str)</p> <p>Name of a dataset that was included in metric filenames and was used for the metrics computation</p> </li> <li> <p>model_names (list)</p> <p>Metrics for what model names to visualize</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attributes names (including attributes intersections),  and values are privilege values for these attributes</p> </li> </ul>"},{"location":"api/custom-classes/MetricsVisualizer/#methods","title":"Methods","text":"create_boxes_and_whiskers_for_models_multiple_runs <p>This boxes and whiskers plot is based on overall subgroup error and stability metrics for all defined models and results after all runs. Using it, you can see combined information on one plot that includes different models,  subgroup metrics, and results after multiple runs.</p> <p>Parameters</p> <ul> <li>metrics_lst     (list)    </li> </ul> create_disparity_metric_heatmap <p>Create a heatmap for disparity metrics.</p> <p>Parameters</p> <ul> <li>model_names     (list)       Metrics for what model names to visualize</li> <li>metrics_lst     (list)    </li> <li>groups_lst     (list)    </li> <li>tolerance     (float)     \u2013 defaults to <code>0.001</code> </li> <li>figsize_scale     (tuple)     \u2013 defaults to <code>(0.7, 0.5)</code> </li> <li>font_increase     (int)     \u2013 defaults to <code>-3</code> </li> </ul> create_overall_metric_heatmap <p>Create a heatmap for overall metrics.</p> <p>Parameters</p> <ul> <li>model_names     (list)       Metrics for what model names to visualize</li> <li>metrics_lst     (list)    </li> <li>tolerance     (float)     \u2013 defaults to <code>0.001</code> </li> <li>figsize_scale     (tuple)     \u2013 defaults to <code>(0.7, 0.5)</code> </li> <li>font_increase     (int)     \u2013 defaults to <code>-3</code> </li> </ul> create_overall_metrics_bar_char <p>This bar chart includes all defined models and all overall subgroup error and stability metrics, which are averaged across multiple runs. Using it, you can compare all models for each subgroup error or stability metric. This comparison also includes reversed metrics, in which values closer to zero are better since straight and reversed metrics in this plot are converted to the same format -- values closer to one are better.</p> <p>Parameters</p> <ul> <li>metric_names     (list)    </li> <li>plot_title     (str)     \u2013 defaults to <code>Overall Metrics</code> </li> </ul>"},{"location":"api/datasets/ACSEmploymentDataset/","title":"ACSEmploymentDataset","text":"<p>Dataset class for the employment task from the folktables dataset. Target: binary classification, predict if a person is employed. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSEmploymentDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSEmploymentDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSIncomeDataset/","title":"ACSIncomeDataset","text":"<p>Dataset class for the income task from the folktables dataset. Target: binary classification, predict if a person has an annual income &gt; $50,000. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSIncomeDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSIncomeDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSMobilityDataset/","title":"ACSMobilityDataset","text":"<p>Dataset class for the mobility task from the folktables dataset. Target: binary classification, predict whether a young adult moved addresses in the last year. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSMobilityDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> </ul>"},{"location":"api/datasets/ACSMobilityDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSPublicCoverageDataset/","title":"ACSPublicCoverageDataset","text":"<p>Dataset class for the public coverage task from the folktables dataset. Target: binary classification, predict whether a low-income individual, not eligible for Medicare,     has coverage from public health insurance. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSPublicCoverageDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> <li> <p>with_filter \u2013 defaults to <code>True</code></p> <p>Whether to use a folktables filter for this task. Default: True.</p> </li> <li> <p>optimize \u2013 defaults to <code>True</code></p> <p>Whether to optimize the dataset size by downcasting categorical columns. Default: True.</p> </li> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset.</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas.</p> </li> </ul>"},{"location":"api/datasets/ACSPublicCoverageDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/ACSTravelTimeDataset/","title":"ACSTravelTimeDataset","text":"<p>Dataset class for the travel time task from the folktables dataset. Target: binary classification, predict whether a working adult has a travel time to work of greater than 20 minutes. Source of the dataset: https://github.com/socialfoundations/folktables</p>"},{"location":"api/datasets/ACSTravelTimeDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>state</p> <p>State in the US for which to get the data. All states in the US are available.</p> </li> <li> <p>year</p> <p>Year for which to get the data. Five different years of data collection are available: 2014\u20132018 inclusive.</p> </li> <li> <p>root_dir \u2013 defaults to <code>None</code></p> <p>Path to the root directory where to store the extracted dataset or where it is stored.</p> </li> <li> <p>with_nulls \u2013 defaults to <code>False</code></p> <p>Whether to keep nulls in the dataset or replace them on the new categorical class. Default: False.</p> </li> </ul>"},{"location":"api/datasets/ACSTravelTimeDataset/#methods","title":"Methods","text":"update_X_data <p>To save simulated nulls</p> <p>Parameters</p> <ul> <li>X_data </li> </ul>"},{"location":"api/datasets/BankMarketingDataset/","title":"BankMarketingDataset","text":"<p>Dataset class for the Bank Marketing dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/bank-full.csv General description and analysis: https://arxiv.org/pdf/2110.00530.pdf (Section 3.1.5) Broad description: https://archive.ics.uci.edu/dataset/222/bank+marketing</p>"},{"location":"api/datasets/BankMarketingDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> </ul>"},{"location":"api/datasets/CardiovascularDiseaseDataset/","title":"CardiovascularDiseaseDataset","text":"<p>Dataset class for the Cardiovascular Disease dataset that contains sensitive attributes among feature columns. Source and broad description: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset</p>"},{"location":"api/datasets/CardiovascularDiseaseDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> </ul>"},{"location":"api/datasets/CompasDataset/","title":"CompasDataset","text":"<p>Dataset class for the COMPAS dataset that contains sensitive attributes among feature columns.</p>"},{"location":"api/datasets/CompasDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/CompasWithoutSensitiveAttrsDataset/","title":"CompasWithoutSensitiveAttrsDataset","text":"<p>Dataset class for the COMPAS dataset that does not contain sensitive attributes among feature columns  to test blind classifiers</p>"},{"location":"api/datasets/CompasWithoutSensitiveAttrsDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/DiabetesDataset2019/","title":"DiabetesDataset2019","text":"<p>Dataset class for the Diabetes 2019 dataset that contains sensitive attributes among feature columns. Source and broad description: https://www.kaggle.com/datasets/tigganeha4/diabetes-dataset-2019/data</p>"},{"location":"api/datasets/DiabetesDataset2019/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>with_nulls (bool) \u2013 defaults to <code>True</code></p> <p>Whether to keep nulls in the dataset or drop rows with any nulls. Default: True.</p> </li> </ul>"},{"location":"api/datasets/GermanCreditDataset/","title":"GermanCreditDataset","text":"<p>Dataset class for the German Credit dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/german_data_credit.csv General description and analysis: https://arxiv.org/pdf/2110.00530.pdf (Section 3.1.3) Broad description: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data</p>"},{"location":"api/datasets/GermanCreditDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> </ul>"},{"location":"api/datasets/LawSchoolDataset/","title":"LawSchoolDataset","text":"<p>Dataset class for the Law School dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/law_school_clean.csv Description: https://arxiv.org/pdf/2110.00530.pdf</p>"},{"location":"api/datasets/LawSchoolDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/RicciDataset/","title":"RicciDataset","text":"<p>Dataset class for the Ricci dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/ricci_race.csv Description: https://arxiv.org/pdf/2110.00530.pdf</p>"},{"location":"api/datasets/RicciDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset_path \u2013 defaults to <code>None</code></p> <p>[Optional] Path to a file with the data</p> </li> </ul>"},{"location":"api/datasets/StudentPerformancePortugueseDataset/","title":"StudentPerformancePortugueseDataset","text":"<p>Dataset class for the Student Performance Portuguese dataset that contains sensitive attributes among feature columns. Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/student_por_new.csv Description: https://arxiv.org/pdf/2110.00530.pdf (Section 3.4.1)</p>"},{"location":"api/datasets/StudentPerformancePortugueseDataset/#parameters","title":"Parameters","text":"<ul> <li> <p>subsample_size (int) \u2013 defaults to <code>None</code></p> <p>Subsample size to create based on the input dataset</p> </li> <li> <p>subsample_seed (int) \u2013 defaults to <code>None</code></p> <p>Seed for sampling using the sample() method from pandas</p> </li> </ul>"},{"location":"api/metrics/aleatoric-uncertainty/","title":"aleatoric_uncertainty","text":"<p>Compute aleatoric uncertainty as average predictive entropy.</p>"},{"location":"api/metrics/aleatoric-uncertainty/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap.</p> </li> </ul>"},{"location":"api/metrics/confusion-matrix-metrics/","title":"confusion_matrix_metrics","text":"<p>Compute accuracy metrics based on the confusion matrix.</p>"},{"location":"api/metrics/confusion-matrix-metrics/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true</p> <p>True labels.</p> </li> <li> <p>y_preds</p> <p>Predicted labels.</p> </li> </ul>"},{"location":"api/metrics/iqr/","title":"iqr","text":"<p>Compute inter-quantile range (IQR) of predictive variance.</p>"},{"location":"api/metrics/iqr/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap</p> </li> </ul>"},{"location":"api/metrics/jitter/","title":"jitter","text":"<p>Jitter is a stability metric that shows how the base model predictions fluctuate. Values closer to 0 -- perfect stability, values closer to 1 -- extremely bad stability.</p>"},{"location":"api/metrics/jitter/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_labels (pandas.core.frame.DataFrame)</p> <p><code>uq_labels</code> variable from count_prediction_metrics()</p> </li> </ul>"},{"location":"api/metrics/label-stability/","title":"label_stability","text":"<p>Compute per-sample accuracy for each model predictions.</p> <p>Return per_sample_accuracy and label_stability (refer to https://www.osti.gov/servlets/purl/1527311)</p>"},{"location":"api/metrics/label-stability/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>y test dataset</p> </li> <li> <p>uq_labels (pandas.core.frame.DataFrame)</p> <p><code>uq_labels</code> variable from count_prediction_metrics()</p> </li> </ul>"},{"location":"api/metrics/mean-prediction/","title":"mean_prediction","text":"<p>Compute mean predictions of all estimators in the boostrap.</p>"},{"location":"api/metrics/mean-prediction/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap</p> </li> </ul>"},{"location":"api/metrics/overall-uncertainty/","title":"overall_uncertainty","text":"<p>Compute overall uncertainty as predictive entropy.</p>"},{"location":"api/metrics/overall-uncertainty/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap.</p> </li> </ul>"},{"location":"api/metrics/statistical-bias/","title":"statistical_bias","text":"<p>Compute statistical bias.</p>"},{"location":"api/metrics/statistical-bias/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap</p> </li> </ul>"},{"location":"api/metrics/std/","title":"std","text":"<p>Compute standard deviation of predictive variance.</p>"},{"location":"api/metrics/std/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of true labels. Is not used in this function, required for consistency.</p> </li> <li> <p>uq_predict_probas (pandas.core.frame.DataFrame)</p> <p>A pandas dataframe of predictions (probabilities) from all estimators in the bootstrap</p> </li> </ul>"},{"location":"api/preprocessing/get-dummies/","title":"get_dummies","text":"<p>Return a dataset made by one-hot encoding for categorical columns and concatenate with numerical columns.</p>"},{"location":"api/preprocessing/get-dummies/#parameters","title":"Parameters","text":"<ul> <li> <p>data (pandas.core.frame.DataFrame)</p> <p>Dataframe for one-hot encoding</p> </li> <li> <p>categorical_columns (list)</p> <p>List of categorical column names</p> </li> <li> <p>numerical_columns (list)</p> <p>List of numerical column names</p> </li> </ul>"},{"location":"api/preprocessing/make-features-dfs/","title":"make_features_dfs","text":"<p>Return preprocessed train and test feature dataframes after one-hot encoding and standard scaling.</p>"},{"location":"api/preprocessing/make-features-dfs/#parameters","title":"Parameters","text":"<ul> <li> <p>X_train (pandas.core.frame.DataFrame)</p> </li> <li> <p>X_test (pandas.core.frame.DataFrame)</p> </li> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> </li> </ul>"},{"location":"api/preprocessing/preprocess-dataset/","title":"preprocess_dataset","text":"<p>Preprocess an input dataset using sklearn ColumnTransformer. Split the dataset on train and test using test_set_fraction.  Create an instance of BaseFlowDataset.</p>"},{"location":"api/preprocessing/preprocess-dataset/#parameters","title":"Parameters","text":"<ul> <li> <p>data_loader (virny.datasets.base.BaseDataLoader)</p> <p>Instance of BaseDataLoader that contains a target, numerical, and categorical columns.</p> </li> <li> <p>column_transformer (sklearn.compose._column_transformer.ColumnTransformer)</p> <p>Instance of sklearn ColumnTransformer to preprocess categorical and numerical columns.</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>Dictionary of sensitive attribute names and their disadvantaged values.</p> </li> <li> <p>test_set_fraction (float)</p> <p>Fraction from 0 to 1. Used to split the input dataset on the train and test sets.</p> </li> <li> <p>dataset_split_seed (int)</p> <p>Seed for dataset splitting.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-with-config/","title":"compute_metrics_with_config","text":"<p>Compute stability and accuracy metrics for each model in models_config. Arguments are defined as an input config object. Save results in <code>save_results_dir_path</code> folder.</p> <p>Return a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/compute-metrics-with-config/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>save_results_dir_path (str)</p> <p>Location where to save result files with metrics</p> </li> <li> <p>postprocessor \u2013 defaults to <code>None</code></p> <p>[Optional] Postprocessor object to apply to model predictions before metrics computation</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] True, if models in models_config have a predict_proba method and can return probabilities for predictions,  False, otherwise. Note that if it is set to False, only metrics based on labels (not labels and probabilities) will be computed.  Ignored when a postprocessor is not None, and set to False in this case.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported. Currently, verbose works only with notebook_logs_stdout = False.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-with-db-writer/","title":"compute_metrics_with_db_writer","text":"<p>Compute stability and accuracy metrics for each model in models_config. Arguments are defined as an input config object. Save results to a database after each run appending fields and value from custom_tbl_fields_dct and using db_writer_func.</p> <p>Return a dictionary where keys are model names, and values are metrics for sensitive attributes defined in config.</p>"},{"location":"api/user-interfaces/compute-metrics-with-db-writer/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>custom_tbl_fields_dct (dict)</p> <p>Dictionary where keys are column names and values to add to inserted metrics during saving results to a database</p> </li> <li> <p>db_writer_func</p> <p>Python function object has one argument (run_models_metrics_df) and save this metrics df to a target database</p> </li> <li> <p>postprocessor \u2013 defaults to <code>None</code></p> <p>[Optional] Postprocessor object to apply to model predictions before metrics computation</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] True, if models in models_config have a predict_proba method and can return probabilities for predictions,  False, otherwise. Note that if it is set to False, only metrics based on labels (not labels and probabilities) will be computed.  Ignored when a postprocessor is not None, and set to False in this case.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported. Currently, verbose works only with notebook_logs_stdout = False.</p> </li> </ul>"},{"location":"api/user-interfaces/compute-metrics-with-multiple-test-sets/","title":"compute_metrics_with_multiple_test_sets","text":"<p>Compute stability and accuracy metrics for each model in models_config based on dataset.X_test and each extra test set  in extra_test_sets_lst. Arguments are defined as an input config object. Save results to a database after each run   appending fields and value from custom_tbl_fields_dct and using db_writer_func.   Index of each test set is also added as a separate column in out final records in the database   (0 index -- for dataset.X_test, 1 and greater -- for each extra test set in extra_test_sets_lst, keeping the original sequence).</p>"},{"location":"api/user-interfaces/compute-metrics-with-multiple-test-sets/#parameters","title":"Parameters","text":"<ul> <li> <p>dataset (custom_classes.BaseFlowDataset)</p> <p>BaseFlowDataset object that contains all needed attributes like target, features, numerical_columns etc.</p> </li> <li> <p>extra_test_sets_lst</p> <p>List of extra test sets like [(X_test1, y_test1), (X_test2, y_test2), ...] to compute metrics that are not equal to original dataset.X_test and dataset.y_test</p> </li> <li> <p>config</p> <p>Object that contains bootstrap_fraction, dataset_name, n_estimators, sensitive_attributes_dct attributes</p> </li> <li> <p>models_config (dict)</p> <p>Dictionary where keys are model names, and values are initialized models</p> </li> <li> <p>custom_tbl_fields_dct (dict)</p> <p>Dictionary where keys are column names and values to add to inserted metrics during saving results to a database</p> </li> <li> <p>db_writer_func</p> <p>Python function object has one argument (run_models_metrics_df) and save this metrics df to a target database</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] True, if models in models_config have a predict_proba method and can return probabilities for predictions,  False, otherwise. Note that if it is set to False, only metrics based on labels (not labels and probabilities) will be computed.  Ignored when a postprocessor is not None, and set to False in this case.</p> </li> <li> <p>notebook_logs_stdout (bool) \u2013 defaults to <code>False</code></p> <p>[Optional] True, if this interface was execute in a Jupyter notebook,  False, otherwise.</p> </li> <li> <p>verbose (int) \u2013 defaults to <code>0</code></p> <p>[Optional] Level of logs printing. The greater level provides more logs.     As for now, 0, 1, 2 levels are supported. Currently, verbose works only with notebook_logs_stdout = False.</p> </li> </ul>"},{"location":"api/utils/count-prediction-metrics/","title":"count_prediction_metrics","text":"<p>Compute means, stds, iqr, entropy, jitter, label stability, and transform predictions to pd.Dataframe.</p> <p>Return a 1D numpy array of predictions, 2D array of each model prediction for y_test, a data structure of metrics.</p>"},{"location":"api/utils/count-prediction-metrics/#parameters","title":"Parameters","text":"<ul> <li> <p>y_true</p> <p>True labels</p> </li> <li> <p>uq_results</p> <p>2D array of prediction proba for the zero value label by each model</p> </li> <li> <p>with_predict_proba (bool) \u2013 defaults to <code>True</code></p> <p>[Optional] A flag if model can return probabilities for its predictions.  If no, only metrics based on labels (not labels and probabilities) will be computed.</p> </li> </ul>"},{"location":"api/utils/create-test-protected-groups/","title":"create_test_protected_groups","text":"<p>Create protected groups based on a test feature set. Use a disadvantaged group as a reference group.</p> <p>Return a dictionary where keys are subgroup names, and values are X_test row indexes correspondent to this subgroup.</p>"},{"location":"api/utils/create-test-protected-groups/#parameters","title":"Parameters","text":"<ul> <li> <p>X_test (pandas.core.frame.DataFrame)</p> <p>Test feature set</p> </li> <li> <p>init_sensitive_attrs_df (pandas.core.frame.DataFrame)</p> <p>Initial full dataset of sensitive attributes without preprocessing</p> </li> <li> <p>sensitive_attributes_dct (dict)</p> <p>A dictionary where keys are sensitive attribute names (including attributes intersections),  and values are disadvantaged values for these attributes</p> </li> </ul>"},{"location":"api/utils/tune-ML-models/","title":"tune_ML_models","text":"<p>Tune each model on a validation set with GridSearchCV.</p> <p>Return each model with its best hyperparameters that have the highest F1 score and Accuracy.  results_df is a dataframe with metrics and tuned parameters;  models_config is a dict with model tuned params for the metrics computation stage</p>"},{"location":"api/utils/tune-ML-models/#parameters","title":"Parameters","text":"<ul> <li> <p>models_params_for_tuning (dict)</p> <p>A dictionary, where keys are model names and values are a dictionary of hyperparameters and value ranges to tune.</p> </li> <li> <p>base_flow_dataset (custom_classes.BaseFlowDataset)</p> <p>An instance of BaseFlowDataset object. Its train and test sets are used for training and tuning.</p> </li> <li> <p>dataset_name (str)</p> <p>A name of the dataset. Used to save tuned hyperparameters to a csv file with an appropriate filename.</p> </li> <li> <p>n_folds (int) \u2013 defaults to <code>3</code></p> <p>The number of folds for k-fold cross validation.</p> </li> </ul>"},{"location":"api/utils/validate-config/","title":"validate_config","text":"<p>Validate parameters types and values in config yaml file.</p> <p>Extra details: * config_obj.model_setting is an optional argument that defines a type of models to use   to compute fairness and stability metrics. Default: 'batch'. </p> <ul> <li>config_obj.computation_mode is an optional argument that defines a non-default mode for metrics computation.   Currently, only 'error_analysis' mode is supported.</li> </ul>"},{"location":"api/utils/validate-config/#parameters","title":"Parameters","text":"<ul> <li> <p>config_obj</p> <p>Object with parameters defined in a yaml file</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_Use_Case/","title":"Multiple Models Interface","text":"<p>In this example, we are going to conduct a deep performance profiling for 4 models. This demonstration will show how to create input arguments for Virny, how to compute overall and disparity metrics with a metric computation interface, and how to build static visualizations based on the calculated metrics. For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Perform disparity metrics composition using the Metric Composer</p> </li> <li> <p>Create static visualizations using the Metric Visualizer</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\nfrom pprint import pprint\nfrom datetime import datetime, timezone\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs, create_models_config_from_tuned_params_df\nfrom virny.user_interfaces.multiple_models_api import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.utils.model_tuning_utils import tune_ML_models\nfrom virny.datasets.base import BaseDataLoader\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre> <pre><code>models_params_for_tuning = {\n    'DecisionTreeClassifier': {\n        'model': DecisionTreeClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [20, 30],\n            \"min_samples_split\" : [0.1],\n            \"max_features\": ['sqrt'],\n            \"criterion\": [\"gini\", \"entropy\"]\n        }\n    },\n    'LogisticRegression': {\n        'model': LogisticRegression(random_state=MODELS_TUNING_SEED),\n        'params': {\n            'penalty': ['l2'],\n            'C' : [0.0001, 0.1, 1, 100],\n            'solver': ['newton-cg', 'lbfgs'],\n            'max_iter': [250],\n        }\n    },\n    'RandomForestClassifier': {\n        'model': RandomForestClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [6, 10],\n            \"min_samples_leaf\": [1],\n            \"n_estimators\": [50, 100],\n            \"max_features\": [0.6]\n        }\n    },\n    'XGBClassifier': {\n        'model': XGBClassifier(random_state=MODELS_TUNING_SEED, verbosity=0),\n        'params': {\n            'learning_rate': [0.1],\n            'n_estimators': [200],\n            'max_depth': [5, 7],\n            'lambda':  [10, 100]\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nrandom_state: 42\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>class CompasWithoutSensitiveAttrsDataset(BaseDataLoader):\n    \"\"\"\n    Dataset class for COMPAS dataset that does not contain sensitive attributes among feature columns\n     to test blind classifiers\n\n    Parameters\n    ----------\n    subsample_size\n        Subsample size to create based on the input dataset\n\n    \"\"\"\n    def __init__(self, dataset_path, subsample_size: int = None):\n        df = pd.read_csv(dataset_path)\n        if subsample_size:\n            df = df.sample(subsample_size)\n\n        # Initial data types transformation\n        int_columns = ['recidivism', 'age', 'age_cat_25 - 45', 'age_cat_Greater than 45',\n                       'age_cat_Less than 25', 'c_charge_degree_F', 'c_charge_degree_M', 'sex']\n        int_columns_dct = {col: \"int\" for col in int_columns}\n        df = df.astype(int_columns_dct)\n\n        # Define params\n        target = 'recidivism'\n        numerical_columns = ['juv_fel_count', 'juv_misd_count', 'juv_other_count','priors_count']\n        categorical_columns = ['age_cat_25 - 45', 'age_cat_Greater than 45','age_cat_Less than 25',\n                               'c_charge_degree_F', 'c_charge_degree_M']\n\n        super().__init__(\n            full_df=df,\n            target=target,\n            numerical_columns=numerical_columns,\n            categorical_columns=categorical_columns\n        )\n</code></pre> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset(dataset_path=os.path.join('virny', 'datasets', 'data', 'COMPAS.csv'))\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader=data_loader, \n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#tune-models-and-create-a-models-config-for-metrics-computation","title":"Tune models and create a models config for metrics computation","text":"<pre><code>tuned_params_df, models_config = tune_ML_models(models_params_for_tuning, base_flow_dataset, config.dataset_name, n_folds=3)\ntuned_params_df\n</code></pre> <pre><code>2024/06/02, 00:28:22: Tuning DecisionTreeClassifier...\n2024/06/02, 00:28:23: Tuning for DecisionTreeClassifier is finished [F1 score = 0.6554846983071246, Accuracy = 0.6575048862828714]\n\n2024/06/02, 00:28:23: Tuning LogisticRegression...\n2024/06/02, 00:28:23: Tuning for LogisticRegression is finished [F1 score = 0.6483823116804865, Accuracy = 0.6520611566087312]\n\n2024/06/02, 00:28:23: Tuning RandomForestClassifier...\n2024/06/02, 00:28:24: Tuning for RandomForestClassifier is finished [F1 score = 0.6569271025126497, Accuracy = 0.6586904492688075]\n\n2024/06/02, 00:28:24: Tuning XGBClassifier...\n2024/06/02, 00:28:24: Tuning for XGBClassifier is finished [F1 score = 0.6623616224585352, Accuracy = 0.6646105242187331]\n</code></pre> Dataset_Name Model_Name F1_Score Accuracy_Score Model_Best_Params 0 COMPAS_Without_Sensitive_Attributes DecisionTreeClassifier 0.655485 0.657505 {'criterion': 'gini', 'max_depth': 20, 'max_fe... 1 COMPAS_Without_Sensitive_Attributes LogisticRegression 0.648382 0.652061 {'C': 1, 'max_iter': 250, 'penalty': 'l2', 'so... 2 COMPAS_Without_Sensitive_Attributes RandomForestClassifier 0.656927 0.658690 {'max_depth': 10, 'max_features': 0.6, 'min_sa... 3 COMPAS_Without_Sensitive_Attributes XGBClassifier 0.662362 0.664611 {'lambda': 100, 'learning_rate': 0.1, 'max_dep... <pre><code>now = datetime.now(timezone.utc)\ndate_time_str = now.strftime(\"%Y%m%d__%H%M%S\")\ntuned_df_path = os.path.join(ROOT_DIR, 'results', 'models_tuning', f'tuning_results_{config.dataset_name}_{date_time_str}.csv')\ntuned_params_df.to_csv(tuned_df_path, sep=\",\", columns=tuned_params_df.columns, float_format=\"%.4f\", index=False)\n</code></pre> <p>Create models_config from the saved tuned_params_df for higher reliability</p> <pre><code>models_config = create_models_config_from_tuned_params_df(models_params_for_tuning, tuned_df_path)\npprint(models_config)\n</code></pre> <pre><code>{'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=20, max_features='sqrt', min_samples_split=0.1,\n                       random_state=42),\n 'LogisticRegression': LogisticRegression(C=1, max_iter=250, random_state=42, solver='newton-cg'),\n 'RandomForestClassifier': RandomForestClassifier(max_depth=10, max_features=0.6, random_state=42),\n 'XGBClassifier': XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, early_stopping_rounds=None,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, lambda=100, learning_rate=0.1,\n              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=5, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              n_estimators=200, n_jobs=None, num_parallel_tree=None,\n              predictor=None, ...)}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After that we need to input the BaseFlowDataset object, models config, and config yaml to a metric computation interface and execute it. The interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged group. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metric computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(base_flow_dataset, config, models_config, SAVE_RESULTS_DIR_PATH,\n                                          notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall sex_priv sex_dis race_priv race_dis 0 IQR 0.093218 0.092883 0.093302 0.095182 0.091952 1 Overall_Uncertainty 0.899836 0.909407 0.897446 0.896719 0.901847 2 Std 0.076228 0.077296 0.075962 0.075141 0.076929 3 Mean_Prediction 0.520117 0.572049 0.507149 0.581026 0.480839 4 Aleatoric_Uncertainty 0.869944 0.875791 0.868484 0.866015 0.872477 5 Statistical_Bias 0.422194 0.416842 0.423530 0.418523 0.424561 6 Epistemic_Uncertainty 0.029893 0.033616 0.028963 0.030704 0.029369 7 Jitter 0.148098 0.159899 0.145152 0.138860 0.154056 8 Label_Stability 0.786591 0.766825 0.791527 0.801256 0.777134 9 TPR 0.687898 0.573333 0.709596 0.578231 0.737654 10 TNR 0.687179 0.808824 0.650334 0.756554 0.628931 11 PPV 0.639053 0.623188 0.641553 0.566667 0.669468 12 FNR 0.312102 0.426667 0.290404 0.421769 0.262346 13 FPR 0.312821 0.191176 0.349666 0.243446 0.371069 14 Accuracy 0.687500 0.725118 0.678107 0.693237 0.683801 15 F1 0.662577 0.597222 0.673861 0.572391 0.701909 16 Selection-Rate 0.480114 0.327014 0.518343 0.362319 0.556075 17 Sample_Size 1056.000000 211.000000 845.000000 414.000000 642.000000"},{"location":"examples/Multiple_Models_Interface_Use_Case/#disparity-metric-composition","title":"Disparity Metric Composition","text":"<p>To compose disparity metrics, the Metric Composer should be applied. Metric Composer is responsible for the second stage of the model audit. Currently, it computes our custom error disparity, stability disparity, and uncertainty disparity metrics, but extending it for new disparity metrics is very simple. We noticed that more and more disparity metrics have appeared during the last decade, but most of them are based on the same group specific metrics. Hence, such a separation of group specific and disparity metrics computation allows us to experiment with different combinations of group specific metrics and avoid group metrics recomputation for a new set of disparity metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre> <pre><code>models_composed_metrics_df\n</code></pre> Metric sex race sex&amp;race Model_Name 0 Accuracy_Difference -0.047012 -0.009436 -0.039300 DecisionTreeClassifier 1 Aleatoric_Uncertainty_Difference -0.007307 0.006463 0.000802 DecisionTreeClassifier 2 Aleatoric_Uncertainty_Ratio 0.991656 1.007463 1.000922 DecisionTreeClassifier 3 Epistemic_Uncertainty_Difference -0.004654 -0.001335 -0.003381 DecisionTreeClassifier 4 Epistemic_Uncertainty_Ratio 0.861563 0.956510 0.892966 DecisionTreeClassifier ... ... ... ... ... ... 71 Disparate_Impact 1.465176 1.537383 1.596796 XGBClassifier 72 Std_Difference 0.000151 0.002984 0.002995 XGBClassifier 73 Std_Ratio 1.003178 1.065098 1.064903 XGBClassifier 74 Equalized_Odds_TNR -0.076968 -0.101583 -0.123015 XGBClassifier 75 Equalized_Odds_TPR 0.153535 0.152053 0.155233 XGBClassifier <p>76 rows \u00d7 5 columns</p>"},{"location":"examples/Multiple_Models_Interface_Use_Case/#metric-visualization","title":"Metric Visualization","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Aleatoric_Uncertainty', 'Overall_Uncertainty', 'Label_Stability', 'Std', 'IQR', 'Jitter'],\n    plot_title=\"Stability and Uncertainty Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metric_heatmap(\n    model_names=list(models_params_for_tuning.keys()),\n    metrics_lst=visualizer.all_accuracy_metrics + visualizer.all_uncertainty_metrics,\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>visualizer.create_disparity_metric_heatmap(\n    model_names=list(models_params_for_tuning.keys()),\n    metrics_lst=[\n        # Error disparity metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        # Stability disparity metrics\n        'Label_Stability_Difference',\n        'IQR_Difference',\n        'Std_Ratio',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n    tolerance=0.005,\n)\n</code></pre> <p></p>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/","title":"Multiple Models With DB Writer Interface","text":"<p>In this example, we are going to conduct a deep performance profiling for 4 models. For that, we will use <code>compute_metrics_with_db_writer</code> interface that will compute metrics for multiple models and save results in the user database based on the db_writer function. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Perform disparity metrics composition using the Metric Composer</p> </li> <li> <p>Create static visualizations using the Metric Visualizer</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.user_interfaces.multiple_models_with_db_writer_api import compute_metrics_with_db_writer\nfrom virny.utils.custom_initializers import create_config_obj, create_models_metrics_dct_from_database_df\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.datasets import CompasWithoutSensitiveAttrsDataset\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>TEST_SET_FRACTION = 0.2\nDATASET_SPLIT_SEED = 42\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_db_writer</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.getcwd()\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \\\n\"\"\"dataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nrandom_state: 42\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-dataset-class","title":"Create a Dataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#create-a-models-config","title":"Create a models config","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'LogisticRegression': LogisticRegression(C=1,\n                                             max_iter=50,\n                                             penalty='l2',\n                                             solver='newton-cg'),\n    'RandomForestClassifier': RandomForestClassifier(max_depth=4,\n                                                     max_features=0.6,\n                                                     min_samples_leaf=1,\n                                                     n_estimators=50),\n    'XGBClassifier': XGBClassifier(learning_rate=0.1,\n                                   max_depth=5,\n                                   n_estimators=20),\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After that we need to input the BaseFlowDataset object, models config, and config yaml to a metric computation interface and execute it. The interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged group. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metric computation, their metrics are combined, returned in a matrix format, and stored in the user defined database using the input db_writer function.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\n\n\nload_dotenv(os.path.join(ROOT_DIR, 'secrets.env'))  # Take environment variables from .env\n\n# Provide the mongodb atlas url to connect python to mongodb using pymongo\nCONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\nclient = MongoClient(CONNECTION_STRING)\ncollection = client[os.getenv(\"DB_NAME\")]['preprocessing_results']\n\n\ndef db_writer_func(run_models_metrics_df, collection=collection):\n    run_models_metrics_df.columns = run_models_metrics_df.columns.str.lower()  # Rename Pandas columns to lower case\n    collection.insert_many(run_models_metrics_df.to_dict('records'))\n</code></pre> <pre><code>import uuid\n\ncustom_table_fields_dct = {\n    'session_uuid': str(uuid.uuid4()),\n    'preprocessing_techniques': 'get_dummies and scaler',\n}\nprint('Current session uuid: ', custom_table_fields_dct['session_uuid'])\n</code></pre> <pre><code>Current session uuid:  65f2800c-dea8-4760-89bd-40564b4e19fd\n</code></pre> <pre><code>metrics_dct = compute_metrics_with_db_writer(base_flow_dataset, config, models_config, custom_table_fields_dct, db_writer_func,\n                                             notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall sex_priv sex_dis race_priv race_dis 0 Statistical_Bias 0.415777 0.411280 0.416900 0.411460 0.418561 1 Std 0.070086 0.072965 0.069367 0.069672 0.070352 2 Mean_Prediction 0.519189 0.574330 0.505420 0.583615 0.477643 3 Overall_Uncertainty 0.885080 0.894485 0.882731 0.879480 0.888691 4 Aleatoric_Uncertainty 0.859123 0.866579 0.857261 0.853366 0.862836 5 IQR 0.084150 0.081478 0.084817 0.085661 0.083176 6 Epistemic_Uncertainty 0.025957 0.027907 0.025470 0.026114 0.025856 7 Label_Stability 0.854811 0.842275 0.857941 0.865700 0.847788 8 Jitter 0.111783 0.119586 0.109835 0.103488 0.117133 9 TPR 0.656051 0.480000 0.689394 0.517007 0.719136 10 TNR 0.735043 0.808824 0.712695 0.790262 0.688679 11 PPV 0.665948 0.580645 0.679104 0.575758 0.701807 12 FNR 0.343949 0.520000 0.310606 0.482993 0.280864 13 FPR 0.264957 0.191176 0.287305 0.209738 0.311321 14 Accuracy 0.699811 0.691943 0.701775 0.693237 0.704050 15 F1 0.660963 0.525547 0.684211 0.544803 0.710366 16 Selection-Rate 0.439394 0.293839 0.475740 0.318841 0.517134 17 Positive-Rate 0.985138 0.826667 1.015152 0.897959 1.024691 18 Sample_Size 1056.000000 211.000000 845.000000 414.000000 642.000000"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#disparity-metric-composition","title":"Disparity Metric Composition","text":"<p>To compose disparity metrics, the Metric Composer should be applied. Metric Composer is responsible for the second stage of the model audit. Currently, it computes our custom error disparity, stability disparity, and uncertainty disparity metrics, but extending it for new disparity metrics is very simple. We noticed that more and more disparity metrics have appeared during the last decade, but most of them are based on the same group specific metrics. Hence, such a separation of group specific and disparity metrics computation allows us to experiment with different combinations of group specific metrics and avoid group metrics recomputation for a new set of disparity metrics.</p> <pre><code>def read_model_metric_dfs_from_db(collection, session_uuid):\n    cursor = collection.find({'session_uuid': session_uuid})\n    records = []\n    for record in cursor:\n        del record['_id']\n        records.append(record)\n\n    model_metric_dfs = pd.DataFrame(records)\n\n    # Capitalize column names to be consistent across the whole library\n    new_column_names = []\n    for col in model_metric_dfs.columns:\n        new_col_name = '_'.join([c.capitalize() for c in col.split('_')])\n        new_column_names.append(new_col_name)\n\n    model_metric_dfs.columns = new_column_names\n    return model_metric_dfs\n</code></pre> <pre><code>model_metric_dfs = read_model_metric_dfs_from_db(collection, custom_table_fields_dct['session_uuid'])\nmodels_metrics_dct = create_models_metrics_dct_from_database_df(model_metric_dfs)\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_DB_Writer/#metric-visualization","title":"Metric Visualization","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Aleatoric_Uncertainty', 'Overall_Uncertainty', 'Label_Stability', 'Std', 'IQR', 'Jitter'],\n    plot_title=\"Stability and Uncertainty Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metric_heatmap(\n    model_names=list(models_config.keys()),\n    metrics_lst=visualizer.all_accuracy_metrics + visualizer.all_uncertainty_metrics,\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>visualizer.create_disparity_metric_heatmap(\n    model_names=list(models_config.keys()),\n    metrics_lst=[\n        # Error disparity metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        # Stability disparity metrics\n        'Label_Stability_Difference',\n        'Aleatoric_Uncertainty_Difference',\n        'Std_Ratio',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>client.close()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/","title":"Multiple Models Interface With Error Analysis","text":"<p>In this example, we are going to conduct a deep performance profiling for 4 models. The only difference with the multiple models interface tutorial is the use of an <code>error_analysis</code> computation mode. This mode measures subgroup metrics also for correct and incorrect predictions. For example, when a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> <p>For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Perform disparity metrics composition using the Metric Composer</p> </li> <li> <p>Create static visualizations using the Metric Visualizer</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nfrom datetime import datetime, timezone\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs, create_models_config_from_tuned_params_df\nfrom virny.user_interfaces.multiple_models_api import compute_metrics_with_config\nfrom virny.datasets import CompasWithoutSensitiveAttrsDataset\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>computation_mode: str, 'default' or 'error_analysis'. Name of the computation mode. When a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nrandom_state: 42\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#create-a-models-config-for-metrics-computation","title":"Create a models config for metrics computation","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'LogisticRegression': LogisticRegression(penalty='l2',\n                                             C=0.1,\n                                             max_iter=250),\n    'RandomForestClassifier': RandomForestClassifier(max_depth=4,\n                                                     max_features=0.6,\n                                                     min_samples_leaf=1,\n                                                     n_estimators=50),\n    'XGBClassifier': XGBClassifier(learning_rate=0.1,\n                                   n_estimators=200,\n                                   max_depth=5,\n                                   verbosity=0)\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After that we need to input the BaseFlowDataset object, models config, and config yaml to a metric computation interface and execute it. The interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged group. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metric computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(base_flow_dataset, config, models_config, SAVE_RESULTS_DIR_PATH, notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics. Note that now we have metrics also for <code>*_correct</code> and <code>*_incorrect</code> subgroups.</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:5]].head(20)\n</code></pre> Metric overall sex_priv sex_priv_correct sex_priv_incorrect 0 Statistical_Bias 0.416691 0.413261 0.324033 0.618208 1 Overall_Uncertainty 0.887649 0.898580 0.880975 0.939015 2 Aleatoric_Uncertainty 0.859615 0.866990 0.852746 0.899708 3 IQR 0.087474 0.088773 0.081936 0.104479 4 Std 0.073404 0.076654 0.071201 0.089178 5 Mean_Prediction 0.519733 0.575657 0.597694 0.525040 6 Epistemic_Uncertainty 0.028034 0.031589 0.028229 0.039308 7 Jitter 0.108416 0.130465 0.102774 0.194069 8 Label_Stability 0.862917 0.827488 0.866939 0.736875 9 TPR 0.656051 0.493333 1.000000 0.000000 10 TNR 0.733333 0.808824 1.000000 0.000000 11 PPV 0.664516 0.587302 1.000000 0.000000 12 FNR 0.343949 0.506667 0.000000 1.000000 13 FPR 0.266667 0.191176 0.000000 1.000000 14 Accuracy 0.698864 0.696682 1.000000 0.000000 15 F1 0.660256 0.536232 1.000000 0.000000 16 Selection-Rate 0.440341 0.298578 0.251701 0.406250 17 Sample_Size 1056.000000 211.000000 147.000000 64.000000"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#disparity-metric-composition","title":"Disparity Metric Composition","text":"<p>To compose disparity metrics, the Metric Composer should be applied. Metric Composer is responsible for the second stage of the model audit. Currently, it computes our custom error disparity, stability disparity, and uncertainty disparity metrics, but extending it for new disparity metrics is very simple. We noticed that more and more disparity metrics have appeared during the last decade, but most of them are based on the same group specific metrics. Hence, such a separation of group specific and disparity metrics computation allows us to experiment with different combinations of group specific metrics and avoid group metrics recomputation for a new set of disparity metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Error_Analysis/#metric-visualization","title":"Metric Visualization","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Aleatoric_Uncertainty', 'Overall_Uncertainty', 'Label_Stability', 'Std', 'IQR', 'Jitter'],\n    plot_title=\"Stability and Uncertainty Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metric_heatmap(\n    model_names=list(models_metrics_dct.keys()),\n    metrics_lst=visualizer.all_accuracy_metrics + visualizer.all_stability_metrics,\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>visualizer.create_disparity_metric_heatmap(\n    model_names=list(models_metrics_dct.keys()),\n    metrics_lst=[\n        # Error disparity metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        # Stability disparity metrics\n        'Label_Stability_Difference',\n        'Aleatoric_Uncertainty_Difference',\n        'Std_Ratio',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n    tolerance=0.005,\n)\n</code></pre> <p></p>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/","title":"Multiple Models Interface With Inprocessor","text":"<p>In this example, we are going to audit one inprocessor from AIF360 for stability and fairness, visualize metrics, and create an analysis report. For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Make group metrics composition</p> </li> <li> <p>Create metrics visualizations and an analysis report</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nfrom pprint import pprint\nfrom datetime import datetime, timezone\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs\nfrom virny.user_interfaces.multiple_models_api import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metrics computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>computation_mode: str, 'default' or 'error_analysis'. Name of the computation mode. When a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: Law_School\nbootstrap_fraction: 0.8\nrandom_state: 42\ncomputation_mode: error_analysis\nn_estimators: 30  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'male': '0', 'race': 'Non-White', 'male&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<pre><code>from virny.datasets import LawSchoolDataset\n\ndata_loader = LawSchoolDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> decile1b decile3 lsat ugpa zfygpa 0 10.0 10.0 44.0 3.5 1.33 1 5.0 4.0 29.0 3.5 -0.11 2 8.0 7.0 37.0 3.4 0.63 3 8.0 7.0 43.0 3.3 0.67 4 3.0 2.0 41.0 3.3 -0.67 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code># Create a binary race column for in-processing since aif360 inprocessors use a sensitive attribute during their learning.\ndata_loader.X_data['race_binary'] = data_loader.X_data['race'].apply(lambda x: 1 if x == 'White' else 0)\n\nbase_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\nbase_flow_dataset.X_train_val['race_binary'] = data_loader.X_data.loc[base_flow_dataset.X_train_val.index, 'race_binary']\nbase_flow_dataset.X_test['race_binary'] = data_loader.X_data.loc[base_flow_dataset.X_test.index, 'race_binary']\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#define-an-inprocessor-and-create-a-wrapper-for-it","title":"Define an inprocessor and create a wrapper for it","text":"<p>To use inprocessors from AIF360 together with Virny, we need to create a wrapper to use it as a basic model in the models_config.</p> <p>A wrapper should include the following methods:</p> <ul> <li> <p>fit(self, X, y): fits an inprocessor based on X and y pandas dataframes. Returns self.</p> </li> <li> <p>predict_proba(self, X): predicts using the fitted inprocessor based on X features pandas dataframe. Returns probabilities for ZERO class. These probabilities will be used by Virny in the downstream metric computation.</p> </li> <li> <p>predict(self, X): predicts using the fitted inprocessor based on X features pandas dataframe. Returns labels for each sample.</p> </li> <li> <p>copy(self) and deepcopy(self, memo): methods, which will be used during copy.copy(inprocessor_wrapper) and copy.deepcopy(inprocessor_wrapper). Return a new instance of inprocessor's wrapper.</p> </li> <li> <p>get_params(self): returns parameters of the wrapper. Alignment with sklearn API.</p> </li> </ul> <pre><code>import copy\nimport numpy as np\n\nfrom aif360.algorithms.inprocessing import ExponentiatedGradientReduction\nfrom virny.custom_classes.base_inprocessing_wrapper import BaseInprocessingWrapper\nfrom virny.utils.postprocessing_intervention_utils import construct_binary_label_dataset_from_df\n\n\nclass ExpGradientReductionWrapper(BaseInprocessingWrapper):\n    \"\"\"\n    A wrapper for fair inprocessors from aif360. The wrapper aligns fit, predict, and predict_proba methods\n    to be compatible with sklearn models.\n\n    Parameters\n    ----------\n    inprocessor\n        An initialized inprocessor from aif360.\n    sensitive_attr_for_intervention\n        A sensitive attribute name to use in the fairness in-processing intervention.\n\n    \"\"\"\n\n    def __init__(self, estimator, sensitive_attr_for_intervention):\n        self.sensitive_attr_for_intervention = sensitive_attr_for_intervention\n        self.estimator = estimator\n        self.inprocessor = ExponentiatedGradientReduction(estimator=self.estimator,\n                                                          constraints='DemographicParity',\n                                                          drop_prot_attr=True)\n\n    def __copy__(self):\n        new_estimator = copy.copy(self.estimator)\n        return ExpGradientReductionWrapper(estimator=new_estimator,\n                                           sensitive_attr_for_intervention=self.sensitive_attr_for_intervention)\n\n    def __deepcopy__(self, memo):\n        new_estimator = copy.deepcopy(self.estimator)\n        return ExpGradientReductionWrapper(estimator=new_estimator,\n                                           sensitive_attr_for_intervention=self.sensitive_attr_for_intervention)\n\n    def get_params(self):\n        return {'sensitive_attr_for_intervention': self.sensitive_attr_for_intervention}\n\n    def set_params(self, random_state):\n        self.estimator.set_params(random_state=random_state)\n        self.inprocessor = ExponentiatedGradientReduction(estimator=self.estimator,\n                                                          constraints='DemographicParity',\n                                                          drop_prot_attr=True)\n\n    def fit(self, X, y):\n        train_binary_dataset = construct_binary_label_dataset_from_df(X_sample=X,\n                                                                      y_sample=y,\n                                                                      target_column='target',\n                                                                      sensitive_attribute=self.sensitive_attr_for_intervention)\n        # Fit an inprocessor\n        self.inprocessor.fit(train_binary_dataset)\n        return self\n\n    def predict_proba(self, X):\n        y_empty = np.zeros(X.shape[0])\n        test_binary_dataset = construct_binary_label_dataset_from_df(X_sample=X,\n                                                                     y_sample=y_empty,\n                                                                     target_column='target',\n                                                                     sensitive_attribute=self.sensitive_attr_for_intervention)\n        test_dataset_pred = self.inprocessor.predict(test_binary_dataset)\n        # Set 1.0 since ExponentiatedGradientReduction can return probabilities slightly higher than 1.0.\n        # This can cause Infinity values for entropy.\n        test_dataset_pred.scores[test_dataset_pred.scores &gt; 1.0] = 1.0\n        # Return 1 - test_dataset_pred.scores since scores are probabilities for label 1, not for label 0\n        return 1 - test_dataset_pred.scores\n\n    def predict(self, X):\n        y_empty = np.zeros(shape=X.shape[0])\n        test_binary_dataset = construct_binary_label_dataset_from_df(X_sample=X,\n                                                                     y_sample=y_empty,\n                                                                     target_column='target',\n                                                                     sensitive_attribute=self.sensitive_attr_for_intervention)\n        test_dataset_pred = self.inprocessor.predict(test_binary_dataset)\n        return test_dataset_pred.labels\n</code></pre> <pre><code># Define a name of a sensitive attribute for the in-processing intervention.\n# Note that in the above wrapper, 1 is used as a favorable label, and 0 is used as an unfavorable label.\nsensitive_attr_for_intervention = 'race_binary'\n\n# Define an estimator\nestimator = LogisticRegression(solver='lbfgs', max_iter=1000)\nmodels_config = {\n    'ExponentiatedGradientReduction': ExpGradientReductionWrapper(estimator=estimator,\n                                                                  sensitive_attr_for_intervention=sensitive_attr_for_intervention)\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#subgroup-metrics-computation","title":"Subgroup Metrics Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(dataset=base_flow_dataset,\n                                          config=config,\n                                          models_config=models_config,\n                                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n                                          notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/30 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall male_priv male_priv_correct male_priv_incorrect male_dis 0 Aleatoric_Uncertainty 0.005905 0.004883 0.003296 0.021364 0.007256 1 IQR 0.010355 0.009922 0.008073 0.029115 0.010926 2 Mean_Prediction 0.024633 0.021842 0.015440 0.088320 0.028322 3 Overall_Uncertainty 0.020169 0.018285 0.012946 0.073729 0.022659 4 Statistical_Bias 0.098458 0.089847 0.004210 0.979146 0.109838 5 Std 0.009615 0.008868 0.006229 0.036276 0.010603 6 Epistemic_Uncertainty 0.014264 0.013402 0.009650 0.052365 0.015403 7 Label_Stability 0.989087 0.989696 0.992222 0.963462 0.988281 8 Jitter 0.008198 0.007553 0.005556 0.028294 0.009050 9 TPR 0.990612 0.991163 1.000000 0.000000 0.989861 10 TNR 0.141204 0.133028 1.000000 0.000000 0.149533 11 PPV 0.908711 0.918534 1.000000 0.000000 0.895642 12 FNR 0.009388 0.008837 0.000000 1.000000 0.010139 13 FPR 0.858796 0.866972 0.000000 1.000000 0.850467 14 Accuracy 0.902404 0.912162 1.000000 0.000000 0.889509 15 F1 0.947895 0.953468 1.000000 0.000000 0.940397 16 Selection-Rate 0.976923 0.979730 0.986574 0.908654 0.973214 17 Sample_Size 4160.000000 2368.000000 2160.000000 208.000000 1792.000000"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#group-metrics-composition","title":"Group Metrics Composition","text":"<p>Metrics Composer is responsible for this second stage of the model audit. Currently, it computes our custom group fairness and stability metrics, but extending it for new group metrics is very simple. We noticed that more and more group metrics have appeared during the last decade, but most of them are based on the same subgroup metrics. Hence, such a separation of subgroup and group metrics computation allows one to experiment with different combinations of subgroup metrics and avoid subgroup metrics recomputation for a new set of grouped metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre> <pre><code>models_composed_metrics_df\n</code></pre> Metric male race male&amp;race Model_Name 0 Accuracy_Difference -0.022653 -0.178877 -0.157307 ExponentiatedGradientReduction 1 Aleatoric_Uncertainty_Difference 0.002373 0.018372 0.021097 ExponentiatedGradientReduction 2 Aleatoric_Uncertainty_Ratio 1.485922 6.916304 5.985519 ExponentiatedGradientReduction 3 Epistemic_Uncertainty_Difference 0.002001 0.009870 0.014769 ExponentiatedGradientReduction 4 Epistemic_Uncertainty_Ratio 1.149317 1.773535 2.128039 ExponentiatedGradientReduction 5 Equalized_Odds_FNR 0.001302 0.001559 0.003110 ExponentiatedGradientReduction 6 Equalized_Odds_FPR -0.016505 0.076428 0.045638 ExponentiatedGradientReduction 7 IQR_Difference 0.001005 0.010219 0.012572 ExponentiatedGradientReduction 8 Jitter_Difference 0.001498 0.009698 0.013220 ExponentiatedGradientReduction 9 Label_Stability_Ratio 0.998571 0.988954 0.984495 ExponentiatedGradientReduction 10 Label_Stability_Difference -0.001415 -0.010944 -0.015355 ExponentiatedGradientReduction 11 Overall_Uncertainty_Difference 0.004374 0.028242 0.035866 ExponentiatedGradientReduction 12 Overall_Uncertainty_Ratio 1.239210 2.780147 3.070293 ExponentiatedGradientReduction 13 Statistical_Parity_Difference -0.006515 -0.011852 -0.014432 ExponentiatedGradientReduction 14 Disparate_Impact 0.993350 0.987890 0.985245 ExponentiatedGradientReduction 15 Std_Difference 0.001734 0.009583 0.013289 ExponentiatedGradientReduction 16 Std_Ratio 1.195550 2.175074 2.552202 ExponentiatedGradientReduction 17 Equalized_Odds_TNR 0.016505 -0.076428 -0.045638 ExponentiatedGradientReduction 18 Equalized_Odds_TPR -0.001302 -0.001559 -0.003110 ExponentiatedGradientReduction"},{"location":"examples/Multiple_Models_Interface_With_Inprocessor/#metrics-visualization-and-reporting","title":"Metrics Visualization and Reporting","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Aleatoric_Uncertainty', 'Epistemic_Uncertainty', 'Std', 'IQR', 'Jitter'],\n    plot_title=\"Stability and Uncertainty Metrics\"\n)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/","title":"Multiple Models Interface For Multiple Test Sets","text":"<p>In this example, we are going to conduct a deep performance profiling for 2 models under multiple test sets. It turns out particularly effective in case we want to test models on different test sets, but to avoid an overhead with retraining a bootstrap with 50-200 estimators. For that, we will use <code>compute_metrics_with_multiple_test_sets</code> interface that will run metric computation for multiple models and test each model using multiple test sets.</p>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nimport pandas as pd\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.user_interfaces.multiple_models_with_multiple_test_sets_api import compute_metrics_with_multiple_test_sets\nfrom virny.utils.custom_initializers import create_config_obj, create_models_metrics_dct_from_database_df\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.datasets import CompasWithoutSensitiveAttrsDataset\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>TEST_SET_FRACTION = 0.2\nDATASET_SPLIT_SEED = 42\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_multiple_test_sets</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>computation_mode: str, 'default' or 'error_analysis'. Name of the computation mode. When a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.getcwd()\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \\\n\"\"\"dataset_name: COMPAS_Without_Sensitive_Attributes\nbootstrap_fraction: 0.8\nrandom_state: 42\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-dataset-class","title":"Create a Dataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = CompasWithoutSensitiveAttrsDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> juv_fel_count juv_misd_count juv_other_count priors_count age_cat_25 - 45 0 0.0 -2.340451 1.0 -15.010999 1 1 0.0 0.000000 0.0 0.000000 1 2 0.0 0.000000 0.0 0.000000 0 3 0.0 0.000000 0.0 6.000000 1 4 0.0 0.000000 0.0 7.513697 1 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#create-a-models-config","title":"Create a models config","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>models_config = {\n    'DecisionTreeClassifier': DecisionTreeClassifier(criterion='gini',\n                                                     max_depth=20,\n                                                     max_features=0.6,\n                                                     min_samples_split=0.1),\n    'RandomForestClassifier': RandomForestClassifier(max_depth=4,\n                                                     max_features=0.6,\n                                                     min_samples_leaf=1,\n                                                     n_estimators=50),\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Multiple_Test_Sets/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After that we need to input the BaseFlowDataset object, models config, and config yaml to a metric computation interface and execute it. The interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged group. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metric computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom pymongo import MongoClient\n\n\nload_dotenv(os.path.join(ROOT_DIR, 'secrets.env'))  # Take environment variables from .env\n\n# Provide the mongodb atlas url to connect python to mongodb using pymongo\nCONNECTION_STRING = os.getenv(\"CONNECTION_STRING\")\n# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\nclient = MongoClient(CONNECTION_STRING)\ncollection = client[os.getenv(\"DB_NAME\")]['preprocessing_results']\n\n\ndef db_writer_func(run_models_metrics_df, collection=collection):\n    run_models_metrics_df.columns = run_models_metrics_df.columns.str.lower()  # Rename Pandas columns to lower case\n    collection.insert_many(run_models_metrics_df.to_dict('records'))\n</code></pre> <pre><code>import uuid\n\ncustom_table_fields_dct = {\n    'session_uuid': str(uuid.uuid4()),\n    'preprocessing_techniques': 'one hot encoder and scaler',\n}\nprint('Current session uuid: ', custom_table_fields_dct['session_uuid'])\n</code></pre> <pre><code>Current session uuid:  8d31eaab-5d6d-4830-9b23-c29355efa90b\n</code></pre> <pre><code>extra_test_sets_lst = [(base_flow_dataset.X_test, base_flow_dataset.y_test, base_flow_dataset.init_sensitive_attrs_df)]\ncompute_metrics_with_multiple_test_sets(dataset=base_flow_dataset,\n                                        extra_test_sets_lst=extra_test_sets_lst,\n                                        config=config,\n                                        models_config=models_config,\n                                        custom_tbl_fields_dct=custom_table_fields_dct,\n                                        db_writer_func=db_writer_func)\n</code></pre> <pre><code>Analyze multiple models:   0%|\u001b[31m          \u001b[0m| 0/2 [00:00&lt;?, ?it/s]\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 50/50 [00:00&lt;00:00, 112.87it/s]\nAnalyze multiple models:  50%|\u001b[31m\u2588\u2588\u2588\u2588\u2588     \u001b[0m| 1/2 [00:06&lt;00:06,  6.70s/it]\nClassifiers testing by bootstrap: 100%|\u001b[34m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 50/50 [00:03&lt;00:00, 16.63it/s]\nAnalyze multiple models: 100%|\u001b[31m\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u001b[0m| 2/2 [00:16&lt;00:00,  8.05s/it]\n</code></pre> <pre><code>def read_model_metric_dfs_from_db(collection, session_uuid):\n    cursor = collection.find({'session_uuid': session_uuid})\n    records = []\n    for record in cursor:\n        del record['_id']\n        records.append(record)\n\n    model_metric_dfs = pd.DataFrame(records)\n\n    # Capitalize column names to be consistent across the whole library\n    new_column_names = []\n    for col in model_metric_dfs.columns:\n        new_col_name = '_'.join([c.capitalize() for c in col.split('_')])\n        new_column_names.append(new_col_name)\n\n    model_metric_dfs.columns = new_column_names\n    return model_metric_dfs\n</code></pre> <pre><code>model_metric_dfs = read_model_metric_dfs_from_db(collection, custom_table_fields_dct['session_uuid'])\nmodels_metrics_dct = create_models_metrics_dct_from_database_df(model_metric_dfs)\n</code></pre> <pre><code>client.close()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/","title":"Multiple Models Interface With Postprocessor","text":"<p>In this example, we are going to audit 2 models together with a postprocessor from AIF360, visualize metrics, and create an analysis report. For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Perform disparity metrics composition using the Metric Composer</p> </li> <li> <p>Create static visualizations using the Metric Visualizer</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nfrom pprint import pprint\nfrom datetime import datetime, timezone\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom aif360.algorithms.postprocessing import EqOddsPostprocessing\n\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs, create_models_config_from_tuned_params_df\nfrom virny.user_interfaces.multiple_models_api import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\nfrom virny.utils.model_tuning_utils import tune_ML_models\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre> <pre><code>models_params_for_tuning = {\n    'LogisticRegression': {\n        'model': LogisticRegression(random_state=MODELS_TUNING_SEED),\n        'params': {\n            'penalty': ['l2'],\n            'C' : [0.0001, 0.1, 1, 100],\n            'solver': ['newton-cg', 'lbfgs'],\n            'max_iter': [250],\n        }\n    },\n    'RandomForestClassifier': {\n        'model': RandomForestClassifier(random_state=MODELS_TUNING_SEED),\n        'params': {\n            \"max_depth\": [6, 10],\n            \"min_samples_leaf\": [1],\n            \"n_estimators\": [50, 100],\n            \"max_features\": [0.6]\n        }\n    },\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>computation_mode: str, 'default' or 'error_analysis'. Name of the computation mode. When a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> <li> <p>postprocessing_sensitive_attribute: str, a name of a sensitive attribute to use for postprocessing.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\ndataset_name: Law_School\nbootstrap_fraction: 0.8\ncomputation_mode: error_analysis\nrandom_state: 42\nn_estimators: 50  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'male': '0', 'race': 'Non-White', 'male&amp;race': None}\npostprocessing_sensitive_attribute: 'race_binary'\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#preprocess-the-dataset-create-a-baseflowdataset-class-and-define-a-postprocessor","title":"Preprocess the dataset, create a BaseFlowDataset class, and define a postprocessor","text":"<pre><code>from virny.datasets import LawSchoolDataset\n\ndata_loader = LawSchoolDataset()\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> decile1b decile3 lsat ugpa zfygpa 0 10.0 10.0 44.0 3.5 1.33 1 5.0 4.0 29.0 3.5 -0.11 2 8.0 7.0 37.0 3.4 0.63 3 8.0 7.0 43.0 3.3 0.67 4 3.0 2.0 41.0 3.3 -0.67 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code># Create a binary race column for postprocessing since aif360 postprocessors can postprocess a dataset only based on binary columns.\ndata_loader.X_data['race_binary'] = data_loader.X_data['race'].apply(lambda x: 1 if x == 'White' else 0)\n\nbase_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\nbase_flow_dataset.X_train_val['race_binary'] = data_loader.X_data.loc[base_flow_dataset.X_train_val.index, 'race_binary']\nbase_flow_dataset.X_test['race_binary'] = data_loader.X_data.loc[base_flow_dataset.X_test.index, 'race_binary']\n</code></pre> <pre><code># Define a postprocessor\nprivileged_groups = [{'race_binary': 1}]\nunprivileged_groups = [{'race_binary': 0}]\npostprocessor = EqOddsPostprocessing(\n    privileged_groups=privileged_groups,\n    unprivileged_groups=unprivileged_groups,\n    seed=None  # Set postprocessor's seed to None to avoid similar predictions during the bootstrap\n)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#tune-models-and-create-a-models-config-for-metrics-computation","title":"Tune models and create a models config for metrics computation","text":"<pre><code>tuned_params_df, models_config = tune_ML_models(models_params_for_tuning, base_flow_dataset, config.dataset_name, n_folds=3)\ntuned_params_df\n</code></pre> <pre><code>2024/06/02, 00:35:52: Tuning LogisticRegression...\n2024/06/02, 00:35:54: Tuning for LogisticRegression is finished [F1 score = 0.6563618630035558, Accuracy = 0.8987258083904316]\n\n2024/06/02, 00:35:54: Tuning RandomForestClassifier...\n2024/06/02, 00:35:56: Tuning for RandomForestClassifier is finished [F1 score = 0.6538551003755212, Accuracy = 0.8980646712345234]\n</code></pre> Dataset_Name Model_Name F1_Score Accuracy_Score Model_Best_Params 0 Law_School LogisticRegression 0.656362 0.898726 {'C': 100, 'max_iter': 250, 'penalty': 'l2', '... 1 Law_School RandomForestClassifier 0.653855 0.898065 {'max_depth': 10, 'max_features': 0.6, 'min_sa... <pre><code>now = datetime.now(timezone.utc)\ndate_time_str = now.strftime(\"%Y%m%d__%H%M%S\")\ntuned_df_path = os.path.join(ROOT_DIR, 'results', 'models_tuning', f'tuning_results_{config.dataset_name}_{date_time_str}.csv')\ntuned_params_df.to_csv(tuned_df_path, sep=\",\", columns=tuned_params_df.columns, float_format=\"%.4f\", index=False)\n</code></pre> <p>Create models_config from the saved tuned_params_df for higher reliability</p> <pre><code>models_config = create_models_config_from_tuned_params_df(models_params_for_tuning, tuned_df_path)\npprint(models_config)\n</code></pre> <pre><code>{'LogisticRegression': LogisticRegression(C=100, max_iter=250, random_state=42, solver='newton-cg'),\n 'RandomForestClassifier': RandomForestClassifier(max_depth=10, max_features=0.6, n_estimators=50,\n                       random_state=42)}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After the variables are input to a user interface, the interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged subgroup. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metrics computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(dataset=base_flow_dataset,\n                                          config=config,\n                                          models_config=models_config,\n                                          save_results_dir_path=SAVE_RESULTS_DIR_PATH,\n                                          postprocessor=postprocessor,\n                                          notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\nEnabled a postprocessing mode\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n\n\nEnabled a postprocessing mode\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/50 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:6]].head(20)\n</code></pre> Metric overall male_priv male_priv_correct male_priv_incorrect male_dis 0 Jitter 0.044141 0.040939 0.035644 0.094502 0.048374 1 Label_Stability 0.949913 0.953970 0.961893 0.873803 0.944554 2 TPR 0.994903 0.994884 1.000000 0.000000 0.994930 3 TNR 0.078704 0.073394 1.000000 0.000000 0.084112 4 PPV 0.903092 0.913712 1.000000 0.000000 0.889015 5 FNR 0.005097 0.005116 0.000000 1.000000 0.005070 6 FPR 0.921296 0.926606 0.000000 1.000000 0.915888 7 Accuracy 0.899760 0.910051 1.000000 0.000000 0.886161 8 F1 0.946777 0.952572 1.000000 0.000000 0.938995 9 Selection-Rate 0.987260 0.988598 0.992575 0.948357 0.985491 10 Sample_Size 4160.000000 2368.000000 2155.000000 213.000000 1792.000000"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#disparity-metric-composition","title":"Disparity Metric Composition","text":"<p>Metrics Composer is responsible for this second stage of the model audit. Currently, it computes our custom group fairness and stability metrics, but extending it for new group metrics is very simple. We noticed that more and more group metrics have appeared during the last decade, but most of them are based on the same subgroup metrics. Hence, such a separation of subgroup and group metrics computation allows one to experiment with different combinations of subgroup metrics and avoid subgroup metrics recomputation for a new set of grouped metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre> <pre><code>models_composed_metrics_df\n</code></pre> Metric male race male&amp;race Model_Name 0 Accuracy_Difference -0.023890 -0.196227 -0.174183 LogisticRegression 1 Equalized_Odds_FNR -0.000047 -0.005823 -0.005454 LogisticRegression 2 Equalized_Odds_FPR -0.010718 0.129278 0.098266 LogisticRegression 3 Jitter_Difference 0.007435 0.034351 0.049795 LogisticRegression 4 Label_Stability_Ratio 0.990130 0.943974 0.924259 LogisticRegression 5 Label_Stability_Difference -0.009416 -0.053678 -0.072383 LogisticRegression 6 Statistical_Parity_Difference -0.003107 0.015031 0.013838 LogisticRegression 7 Disparate_Impact 0.996857 1.015261 1.014032 LogisticRegression 8 Equalized_Odds_TNR 0.010718 -0.129278 -0.098266 LogisticRegression 9 Equalized_Odds_TPR 0.000047 0.005823 0.005454 LogisticRegression 10 Accuracy_Difference -0.020693 -0.158407 -0.134267 RandomForestClassifier 11 Equalized_Odds_FNR 0.004134 0.020908 0.029136 RandomForestClassifier 12 Equalized_Odds_FPR -0.058218 -0.104439 -0.140207 RandomForestClassifier 13 Jitter_Difference 0.009800 0.093877 0.101423 RandomForestClassifier 14 Label_Stability_Ratio 0.981678 0.844858 0.825698 RandomForestClassifier 15 Label_Stability_Difference -0.017405 -0.149755 -0.166575 RandomForestClassifier 16 Statistical_Parity_Difference -0.013514 -0.061529 -0.076446 RandomForestClassifier 17 Disparate_Impact 0.986242 0.937586 0.922193 RandomForestClassifier 18 Equalized_Odds_TNR 0.058218 0.104439 0.140207 RandomForestClassifier 19 Equalized_Odds_TPR -0.004134 -0.020908 -0.029136 RandomForestClassifier"},{"location":"examples/Multiple_Models_Interface_With_Postprocessor/#metric-visualization","title":"Metric Visualization","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Label_Stability', 'Jitter'],\n    plot_title=\"Stability Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metric_heatmap(\n    model_names=list(models_params_for_tuning.keys()),\n    metrics_lst=['Accuracy', 'F1', 'TNR', 'TPR', 'FNR', 'FPR', 'Label_Stability', 'Jitter'],\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>visualizer.create_disparity_metric_heatmap(\n    model_names=list(models_params_for_tuning.keys()),\n    metrics_lst=[\n        # Error disparity metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        # Stability disparity metrics\n        'Label_Stability_Difference',\n        'Jitter_Difference',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n    tolerance=0.005,\n)\n</code></pre> <p></p>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/","title":"Multiple Models Interface With PyTorch Tabular","text":"<p>In this example, we are going to conduct a performance profiling for 1 deep learning model from PyTorch Tabular. For that, we will use <code>compute_metrics_with_config</code> interface that can compute metrics for multiple models. Thus, we will need to do the next steps:</p> <ul> <li> <p>Initialize input variables</p> </li> <li> <p>Compute subgroup metrics</p> </li> <li> <p>Perform disparity metrics composition using the Metric Composer</p> </li> <li> <p>Create static visualizations using the Metric Visualizer</p> </li> </ul>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#import-dependencies","title":"Import dependencies","text":"<pre><code>import os\nfrom datetime import datetime, timezone\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nfrom virny.datasets import DiabetesDataset2019\nfrom virny.utils.custom_initializers import create_config_obj, read_model_metric_dfs\nfrom virny.user_interfaces.multiple_models_api import compute_metrics_with_config\nfrom virny.preprocessing.basic_preprocessing import preprocess_dataset\nfrom virny.custom_classes.metrics_visualizer import MetricsVisualizer\nfrom virny.custom_classes.metrics_composer import MetricsComposer\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#initialize-input-variables","title":"Initialize Input Variables","text":"<p>Based on the library flow, we need to create 3 input objects for a user interface:</p> <ul> <li> <p>A config yaml that is a file with configuration parameters for different user interfaces for metric computation.</p> </li> <li> <p>A dataset class that is a wrapper above the user\u2019s raw dataset that includes its descriptive attributes like a target column, numerical columns, categorical columns, etc. This class must be inherited from the BaseDataset class, which was created for user convenience.</p> </li> <li> <p>Finally, a models config that is a Python dictionary, where keys are model names and values are initialized models for analysis. This dictionary helps conduct audits for different analysis modes and analyze different types of models.</p> </li> </ul> <pre><code>DATASET_SPLIT_SEED = 42\nMODELS_TUNING_SEED = 42\nTEST_SET_FRACTION = 0.2\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#create-a-config-object","title":"Create a config object","text":"<p><code>compute_metrics_with_config</code> interface requires that your yaml file includes the following parameters:</p> <ul> <li> <p>dataset_name: str, a name of your dataset; it will be used to name files with metrics.</p> </li> <li> <p>bootstrap_fraction: float, the fraction from a train set in the range [0.0 - 1.0] to fit models in bootstrap (usually more than 0.5).</p> </li> <li> <p>random_state: int, a seed to control the randomness of the whole model evaluation pipeline.</p> </li> <li> <p>n_estimators: int, the number of estimators for bootstrap to compute subgroup stability metrics.</p> </li> <li> <p>computation_mode: str, 'default' or 'error_analysis'. Name of the computation mode. When a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect). Therefore, a user can analyze how a model is certain about its incorrect predictions.</p> </li> <li> <p>sensitive_attributes_dct: dict, a dictionary where keys are sensitive attribute names (including intersectional attributes), and values are disadvantaged values for these attributes. Intersectional attributes must include '&amp;' between sensitive attributes. You do not need to specify disadvantaged values for intersectional groups since they will be derived from disadvantaged values in sensitive_attributes_dct for each separate sensitive attribute in this intersectional pair.</p> </li> </ul> <p>Note that disadvantaged value in a sensitive attribute dictionary must be the same as in the original dataset. For example, when distinct values of the sex column in the original dataset are 'F' and 'M', and after pre-processing they became 0 and 1 respectively, you still need to set a disadvantaged value as 'F' or 'M' in the sensitive attribute dictionary.</p> <pre><code>ROOT_DIR = os.path.join('docs', 'examples')\nconfig_yaml_path = os.path.join(ROOT_DIR, 'experiment_config.yaml')\nconfig_yaml_content = \"\"\"\nrandom_state: 42\ndataset_name: diabetes\nbootstrap_fraction: 0.8\nn_estimators: 10  # Better to input the higher number of estimators than 100; this is only for this use case example\nsensitive_attributes_dct: {'Gender': 'Female'}\n\"\"\"\n\nwith open(config_yaml_path, 'w', encoding='utf-8') as f:\n    f.write(config_yaml_content)\n</code></pre> <pre><code>config = create_config_obj(config_yaml_path=config_yaml_path)\nSAVE_RESULTS_DIR_PATH = os.path.join(ROOT_DIR, 'results', f'{config.dataset_name}_Metrics_{datetime.now(timezone.utc).strftime(\"%Y%m%d__%H%M%S\")}')\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#preprocess-the-dataset-and-create-a-baseflowdataset-class","title":"Preprocess the dataset and create a BaseFlowDataset class","text":"<p>Based on the BaseDataset class, your dataset class should include the following attributes:</p> <ul> <li> <p>Obligatory attributes: dataset, target, features, numerical_columns, categorical_columns</p> </li> <li> <p>Optional attributes: X_data, y_data, columns_with_nulls</p> </li> </ul> <p>For more details, please refer to the library documentation.</p> <pre><code>data_loader = DiabetesDataset2019(with_nulls=False)\ndata_loader.X_data[data_loader.X_data.columns[:5]].head()\n</code></pre> BMI Sleep SoundSleep Pregnancies Age 0 39.0 8 6 0.0 50-59 1 28.0 8 6 0.0 50-59 2 24.0 6 6 0.0 40-49 3 23.0 8 6 0.0 50-59 4 27.0 8 8 0.0 40-49 <pre><code>column_transformer = ColumnTransformer(transformers=[\n    ('categorical_features', OneHotEncoder(handle_unknown='ignore', sparse_output=False), data_loader.categorical_columns),\n    ('numerical_features', StandardScaler(), data_loader.numerical_columns),\n])\n</code></pre> <pre><code>base_flow_dataset = preprocess_dataset(data_loader=data_loader,\n                                       column_transformer=column_transformer,\n                                       sensitive_attributes_dct=config.sensitive_attributes_dct,\n                                       test_set_fraction=TEST_SET_FRACTION,\n                                       dataset_split_seed=DATASET_SPLIT_SEED)\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#create-a-models-config-for-metrics-computation","title":"Create a models config for metrics computation","text":"<p>models_config is a Python dictionary, where keys are model names and values are initialized models for analysis</p> <pre><code>from pytorch_tabular.models import GANDALFConfig\nfrom pytorch_tabular import TabularModel\nfrom pytorch_tabular.config import (\n    DataConfig,\n    OptimizerConfig,\n    TrainerConfig,\n)\n\ndata_config = DataConfig(\n    target=[\n        data_loader.target\n    ],  # target should always be a list. Multi-targets are only supported for regression. Multi-Task Classification is not implemented\n    continuous_cols=[col for col in base_flow_dataset.X_train_val.columns if col.startswith('numerical_')],\n    categorical_cols=[col for col in base_flow_dataset.X_train_val.columns if col.startswith('categorical_')],\n)\ntrainer_config = TrainerConfig(\n    batch_size=512,\n    max_epochs=10,\n    load_best=False,\n    trainer_kwargs=dict(enable_model_summary=False, # Turning off model summary\n                        log_every_n_steps=None,\n                        enable_progress_bar=False),\n)\noptimizer_config = OptimizerConfig()\nmodel_config = GANDALFConfig(\n    task=\"classification\",\n    gflu_stages=6,\n    gflu_feature_init_sparsity=0.3,\n    gflu_dropout=0.0,\n    learning_rate=1e-3,\n)\n</code></pre> <pre><code>models_config = {\n    'GANDALFClassifier': TabularModel(\n        data_config=data_config,\n        model_config=model_config,\n        optimizer_config=optimizer_config,\n        trainer_config=trainer_config,\n        verbose=False,\n        suppress_lightning_logger=True,\n    ),\n}\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#subgroup-metric-computation","title":"Subgroup Metric Computation","text":"<p>After that we need to input the BaseFlowDataset object, models config, and config yaml to a metric computation interface and execute it. The interface uses subgroup analyzers to compute different sets of metrics for each privileged and disadvantaged group. As for now, our library supports Subgroup Variance Analyzer and Subgroup Error Analyzer, but it is easily extensible to any other analyzers. When the variance and error analyzers complete metric computation, their metrics are combined, returned in a matrix format, and stored in a file if defined.</p> <pre><code>metrics_dct = compute_metrics_with_config(base_flow_dataset, config, models_config, SAVE_RESULTS_DIR_PATH, notebook_logs_stdout=True)\n</code></pre> <pre><code>Analyze multiple models:   0%|          | 0/1 [00:00&lt;?, ?it/s]\n\n\n\nClassifiers testing by bootstrap:   0%|          | 0/10 [00:00&lt;?, ?it/s]\n</code></pre> <p>Look at several columns in top rows of computed metrics. Note that now we have metrics also for <code>*_correct</code> and <code>*_incorrect</code> subgroups.</p> <pre><code>sample_model_metrics_df = metrics_dct[list(models_config.keys())[0]]\nsample_model_metrics_df[sample_model_metrics_df.columns[:5]].head(20)\n</code></pre> Metric overall Gender_priv Gender_dis Model_Name 0 Statistical_Bias 0.295597 0.321831 0.248779 GANDALFClassifier 1 Mean_Prediction 0.738774 0.752824 0.713700 GANDALFClassifier 2 Std 0.086163 0.084164 0.089730 GANDALFClassifier 3 Aleatoric_Uncertainty 0.690577 0.690398 0.690896 GANDALFClassifier 4 IQR 0.105706 0.105639 0.105825 GANDALFClassifier 5 Overall_Uncertainty 0.722770 0.720565 0.726706 GANDALFClassifier 6 Epistemic_Uncertainty 0.032193 0.030167 0.035810 GANDALFClassifier 7 Jitter 0.104850 0.100192 0.113162 GANDALFClassifier 8 Label_Stability 0.851934 0.860345 0.836923 GANDALFClassifier 9 TPR 0.326531 0.212121 0.562500 GANDALFClassifier 10 TNR 0.969697 0.963855 0.979592 GANDALFClassifier 11 PPV 0.800000 0.700000 0.900000 GANDALFClassifier 12 FNR 0.673469 0.787879 0.437500 GANDALFClassifier 13 FPR 0.030303 0.036145 0.020408 GANDALFClassifier 14 Accuracy 0.795580 0.750000 0.876923 GANDALFClassifier 15 F1 0.463768 0.325581 0.692308 GANDALFClassifier 16 Selection-Rate 0.110497 0.086207 0.153846 GANDALFClassifier 17 Sample_Size 181.000000 116.000000 65.000000 GANDALFClassifier"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#disparity-metric-composition","title":"Disparity Metric Composition","text":"<p>To compose disparity metrics, the Metric Composer should be applied. Metric Composer is responsible for the second stage of the model audit. Currently, it computes our custom error disparity, stability disparity, and uncertainty disparity metrics, but extending it for new disparity metrics is very simple. We noticed that more and more disparity metrics have appeared during the last decade, but most of them are based on the same group specific metrics. Hence, such a separation of group specific and disparity metrics computation allows us to experiment with different combinations of group specific metrics and avoid group metrics recomputation for a new set of disparity metrics.</p> <pre><code>models_metrics_dct = read_model_metric_dfs(SAVE_RESULTS_DIR_PATH, model_names=list(models_config.keys()))\n</code></pre> <pre><code>metrics_composer = MetricsComposer(models_metrics_dct, config.sensitive_attributes_dct)\n</code></pre> <p>Compute composed metrics</p> <pre><code>models_composed_metrics_df = metrics_composer.compose_metrics()\n</code></pre>"},{"location":"examples/Multiple_Models_Interface_With_PyTorch_Tabular/#metric-visualization","title":"Metric Visualization","text":"<p>Metric Visualizer allows us to build static visualizations for the computed metrics. It unifies different preprocessing methods for the computed metrics and creates various data formats required for visualizations. Hence, users can simply call methods of the MetricsVisualizer class and get custom plots for diverse metric analysis.</p> <pre><code>visualizer = MetricsVisualizer(models_metrics_dct, models_composed_metrics_df, config.dataset_name,\n                               model_names=list(models_config.keys()),\n                               sensitive_attributes_dct=config.sensitive_attributes_dct)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Accuracy', 'F1', 'TPR', 'TNR', 'PPV', 'Selection-Rate'],\n    plot_title=\"Accuracy Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metrics_bar_char(\n    metric_names=['Aleatoric_Uncertainty', 'Overall_Uncertainty', 'Label_Stability', 'Std', 'IQR', 'Jitter'],\n    plot_title=\"Stability and Uncertainty Metrics\"\n)\n</code></pre> <pre><code>visualizer.create_overall_metric_heatmap(\n    model_names=list(models_metrics_dct.keys()),\n    metrics_lst=visualizer.all_accuracy_metrics + visualizer.all_stability_metrics,\n    tolerance=0.005,\n)\n</code></pre> <p></p> <pre><code>visualizer.create_disparity_metric_heatmap(\n    model_names=list(models_metrics_dct.keys()),\n    metrics_lst=[\n        # Error disparity metrics\n        'Equalized_Odds_TPR',\n        'Equalized_Odds_FPR',\n        'Disparate_Impact',\n        # Stability disparity metrics\n        'Label_Stability_Difference',\n        'Aleatoric_Uncertainty_Difference',\n        'Std_Ratio',\n    ],\n    groups_lst=config.sensitive_attributes_dct.keys(),\n    tolerance=0.005,\n)\n</code></pre> <p></p>"},{"location":"glossary/bootstrap_approach/","title":"Measuring the Model Arbitrariness","text":"<p>Measuring only model accuracy is insufficient for reliable model development since an unstable or uncertain model can also lead to severe consequences in the production environment. Unfortunately, leading approaches to uncertainty quantification are tied to deep learning architectures and use Bayesian neural networks with methods like Monte Carlo Dropout<sup>1</sup><sup>2</sup>. However, deep learning models are not state-of-the-art on tabular datasets and are still outperformed by ensemble and tree-based models<sup>3</sup>. Hence, model-agnostic uncertainty quantification is a major gap. We take a first step in this direction and propose a data-centric approach by bootstrapping over the training set<sup>4</sup>.</p> Figure 1. Data-centric evaluaton of model arbitrariness. <p>Let \\(D(X,Y)\\) be a (training) dataset of features and targets. Let \\(h(D)\\) be the trained model, fit on the full dataset \\(D\\), whose arbitrariness we are interested in quantifying.  We construct multiple training sets \\(D_i\\) by sampling with replacement from the given training set \\(D\\), and then fitting estimators \\(h_i(D_i)\\) (with the same architecture and hyper-parameters as \\(h(D)\\)) on these bootstrapped training sets to construct an \\(\\textit{approximating ensemble}\\). At inference time, we use the approximating ensemble to construct a \\(\\textit{predictive distribution}\\) of class labels, with probabilities, for each test sample \\(x\\) according to each model \\(h_i\\). We will then use this predictive distribution to compute different measures of variation/disagreement/arbitrariness between the predictions of members of the approximating ensemble, and quantify the expected instability of a model trained on the dataset.</p> <p>References</p> <ol> <li> <p>Yarin Gal and Zoubin Ghahramani. \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning.\" international conference on machine learning. PMLR, 2016.\u00a0\u21a9</p> </li> <li> <p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural NetworksfromOverfitting.JournalofMachineLearningResearch15,56(2014),1929\u20131958.\u00a0\u21a9</p> </li> <li> <p>Vadim Borisov, Tobias Leemann, Kathrin Se\u00dfler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2022. Deep neural networks and tabular data: A survey. IEEE Transactions on Neural Networks and Learning Systems (2022).\u00a0\u21a9</p> </li> <li> <p>Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.\u00a0\u21a9</p> </li> </ol>"},{"location":"glossary/disparity_performance_dimensions/","title":"Disparity Performance Dimensions","text":"<p>This page contains short descriptions and mathematical definitions for each disparity metric implemented in Virny.</p>"},{"location":"glossary/disparity_performance_dimensions/#notation","title":"Notation","text":"<p>Let \\(Y\\) be the true outcome, \\(X\\) be the covariates or features, and \\(A\\) be a set of sensitive attributes. To start, we restrict our analysis to the binary classification of group membership, letting \\(A=0\\) denote the disadvantaged group and \\(A=1\\) denote the privileged group. We are interested in constructing a classifier \\(\\hat{Y} = f(X,A)\\) that predicts \\(Y\\) using an appropriate loss function. In fair-ML, we apply additional constraints on the interaction between \\(\\hat{Y}\\), \\(Y\\), and \\(A\\) to ensure that the classifier \\(\\hat{Y}\\) does not discriminate on the basis of sensitive attributes \\(A\\). Various fairness definitions are formalized as distinct constraints, and when the fairness constraint is not satisfied, it is commonly known as the measure of model unfairness, which we will discuss next.</p> <p>We will now rewrite influential fairness measures by expressing them as the difference or ratio between different \\(\\textit{base measures}\\) on the disadvantage (\\(\\textit{dis}\\)) and privileged (\\(\\textit{priv}\\)) groups: \\(\\Delta f = f_{dis} - f_{priv}\\), \\(\\mathcal{Q} f = f_{dis} / f_{priv}\\).</p>"},{"location":"glossary/disparity_performance_dimensions/#error-disparity","title":"Error Disparity","text":""},{"location":"glossary/disparity_performance_dimensions/#equalized-odds","title":"Equalized Odds","text":"<p>Hardt, Price, and Srebro<sup>1</sup> state that a predictor \\(\\hat{Y}\\) satisfies \\(\\textit{equalized odds}\\) with respect to sensitive attribute \\(A\\) and outcome \\(Y\\), if \\(\\hat{Y}\\) and \\(A\\) are independent conditional on \\(Y\\). In our framework, we focus on the following expression of equalized odds:</p> \\[ P(\\hat{Y}=\\hat{y}|A=0,Y=y)=P(\\hat{Y}=\\hat{y}|A=1,Y=y), y \\in \\{0,1\\} \\] <p>For \\(\\hat{Y} = 1\\) (the predicted positive outcome) and \\(Y = 0\\) (the true negative outcome), this fairness constraint requires parity in false positive rates (FPR) across the groups \\(A = 0\\) and \\(A = 1\\). For \\(\\hat{Y} = 0\\) (the predicted negative outcome) and \\(Y = 1\\) (the true positive outcome), the constraint requires parity in false negative rates (FNR). A violation of this constraint (e.g., the disparity in FPR and FNR across groups) is reported as a measure of model unfairness. In our library, we refer to FPR and FNR as the \\(\\textit{base measures}\\), and we say that the fairness criterion of Equalized Odds is \\(\\textit{composed}\\) as the difference between these \\(\\textit{base measures}\\) computed for the disadvantaged group (\\(A=0\\), which we call \\(\\textit{dis}\\)) and for the privileged group (\\(A=1\\), which we call \\(\\textit{priv}\\)), respectively.</p> <p>Equalized Odds Violation (False Positive):</p> \\[ \\Delta\\text{FPR} = P(\\hat{Y}=1|A=0,Y=0)- P(\\hat{Y}=1|A=1,Y=0) \\] <p>Equalized Odds Violation (False Negative):</p> \\[ \\Delta\\text{FNR} = P(\\hat{Y}=0|A=0,Y=1)- P(\\hat{Y}=0|A=1,Y=1) \\]"},{"location":"glossary/disparity_performance_dimensions/#accuracy-difference","title":"Accuracy Difference","text":"<p>Accuracy Difference is a fairness notion that requires equal accuracy across groups.</p> \\[ \\Delta(\\text{Accuracy}) = P(\\hat{Y}=Y|A=0) - P(\\hat{Y}=Y|A=1) \\]"},{"location":"glossary/disparity_performance_dimensions/#representation-disparity","title":"Representation Disparity","text":""},{"location":"glossary/disparity_performance_dimensions/#disparate-impact","title":"Disparate Impact","text":"<p>Inspired by the 4/5th's rule in legal doctrines, Disparate Impact<sup>2</sup><sup>3</sup> has been formulated as a fairness metric:</p> \\[ \\text{Disparate Impact} = \\mathcal{Q}(\\text{Selection Rate}) = \\frac{P(\\hat{Y}=1|A=0)}{P(\\hat{Y}=1|A=1)} \\] <p>\\(P(\\hat{Y}=1)\\) is simply the Selection Rate of the classifier, and so the measure of Disparate Impact is composed as the ratio of the Selection Rate on the \\(\\textit{dis}\\) and \\(\\textit{priv}\\) groups, respectively.</p>"},{"location":"glossary/disparity_performance_dimensions/#statistical-parity-difference","title":"Statistical Parity Difference","text":"<p>Statistical Parity<sup>4</sup><sup>5</sup><sup>6</sup><sup>7</sup> is the fairness notion that asks if comparable proportions of samples from each protected group receive the positive outcome:</p> \\[ P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1) \\] <p>Statistical Parity Difference (SPD) is a popular fairness measure composed simply as the difference between the classifier's Selection Rate on \\(\\textit{dis}\\) and \\(\\textit{priv}\\) groups, respectively.</p> \\[ \\text{Statistical Parity Difference} = \\Delta(\\text{Selection Rate}) = P(\\hat{Y}=1|A=0) - P(\\hat{Y}=1|A=1) \\]"},{"location":"glossary/disparity_performance_dimensions/#stability-disparity","title":"Stability Disparity","text":""},{"location":"glossary/disparity_performance_dimensions/#label-stability-difference","title":"Label Stability Difference","text":"<p>Label Stability Difference measures the equality (or lack thereof) of label stability across groups. In practice, this metric is implemented as a difference between the averaged metric value for \\(\\textit{dis}\\) and \\(\\textit{priv}\\) groups:</p> \\[ \\Delta U_{h, \\text{stability}}(D)  = \\mathbb{E}_{x \\in D_\\text{dis}}[U_{\\text{stability}}(x)] - \\mathbb{E}_{x^* \\in D_\\text{priv}}[U_{\\text{stability}}(x^*)] \\] <p>where \\(D\\) is a test set and \\(x\\) and \\(x^*\\) are \\(dis\\) and \\(priv\\) samples in \\(D\\).</p>"},{"location":"glossary/disparity_performance_dimensions/#uncertainty-disparity","title":"Uncertainty Disparity","text":""},{"location":"glossary/disparity_performance_dimensions/#epistemic-uncertainty-difference","title":"Epistemic Uncertainty Difference","text":"<p>Epistemic Uncertainty Difference measures the equality (or lack thereof) of epistemic (model) uncertainty across groups. In practice, this metric is implemented as a difference between the averaged metric value for \\(\\textit{dis}\\) and \\(\\textit{priv}\\) groups:</p> \\[ \\Delta U_{h, \\text{epistemic}}(D)  = \\mathbb{E}_{x \\in D_\\text{dis}}[U_{\\text{epistemic}}(x)] -  \\mathbb{E}_{x^* \\in D_\\text{priv}}[U_{\\text{epistemic}}(x^*)] \\] <p>where \\(D\\) is a test set and \\(x\\) and \\(x^*\\) are \\(dis\\) and \\(priv\\) samples in \\(D\\).</p>"},{"location":"glossary/disparity_performance_dimensions/#aleatoric-uncertainty-difference","title":"Aleatoric Uncertainty Difference","text":"<p>Aleatoric Uncertainty Difference measures the equality (or lack thereof) of aleatoric (data) uncertainty across groups. In practice, this metric is implemented as a difference between the averaged metric value for \\(\\textit{dis}\\) and \\(\\textit{priv}\\) groups:</p> \\[ \\Delta U_{h, \\text{aleatoric}}(D)  = \\mathbb{E}_{x \\in D_\\text{dis}}[U_{\\text{aleatoric}}(x)] -  \\mathbb{E}_{x^* \\in D_\\text{priv}}[U_{\\text{aleatoric}}(x^*)] \\] <p>where \\(D\\) is a test set and \\(x\\) and \\(x^*\\) are \\(dis\\) and \\(priv\\) samples in \\(D\\).</p> <p>References</p> <ol> <li> <p>Moritz Hardt, Eric Price, and Nati Srebro. \u201cEquality of Opportunity in Supervised Learning\u201d. In: Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain. Ed. by Daniel D. Lee et al. 2016, pp. 3315\u20133323.\u00a0\u21a9</p> </li> <li> <p>Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big Data 5, 2 (2017), 153\u2013163. 2016\u00a0\u21a9</p> </li> <li> <p>Feldman, Michael, et al. \"Certifying and removing disparate impact.\" proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. 2015.\u00a0\u21a9</p> </li> <li> <p>Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2):277\u2013292, 2010.\u00a0\u21a9</p> </li> <li> <p>Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubramanian. Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 259\u2013268, New York, NY, USA, 2015. ACM.\u00a0\u21a9</p> </li> <li> <p>Faisal Kamiran and Toon Calders. Classifying without discriminating. 2009 2nd International Conference on Computer, Control and Communication, IC4 2009, 2009.\u00a0\u21a9</p> </li> <li> <p>Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regularization approach. Proceedings - IEEE International Conference on Data Mining, ICDM, pages 643\u2013650, 2011.\u00a0\u21a9</p> </li> </ol>"},{"location":"glossary/overall_performance_dimensions/","title":"Overall Performance Dimensions","text":"<p>This page contains short descriptions and mathematical definitions for each overall metric implemented in Virny.</p>"},{"location":"glossary/overall_performance_dimensions/#correctness","title":"Correctness","text":""},{"location":"glossary/overall_performance_dimensions/#accuracy","title":"Accuracy","text":"<p>Accuracy<sup>1</sup> is the fraction of correct predictions. If \\(\\hat{Y}_i\\) is the predicted value of the \\(i\\)-th sample and \\(Y_i\\) is the corresponding true value, then the fraction of correct predictions over \\(n_{samples}\\) is defined as:</p> \\[ \\text{Accuracy } (Y, \\hat{Y}) = \\frac{1}{n_{samples}} \\sum_{i=1}^{n_{samples}} \\mathbf{1}(\\hat{Y}_i = Y_i) \\]"},{"location":"glossary/overall_performance_dimensions/#f1-score","title":"F1 Score","text":"<p>The F1 score<sup>2</sup> can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches  its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal.  The formula for the F1 score is:</p> \\[ F_1 = \\frac{2}{\\text{ recall }^{-1} + \\text{ precision }^{-1}} = 2 \\frac{\\text{ precision } \\cdot \\text{ recall }}{\\text{ precision } + \\text{ recall }} = \\frac{2 \\mathrm{TP}}{2 \\mathrm{TP}+\\mathrm{FP}+\\mathrm{FN}} \\] <p>where \\(TP\\) is the number of true positives, \\(FN\\) is the number of false negatives, and \\(FP\\) is the number of false positives.  F1 is by default calculated as 0.0 when there are no true positives, false negatives, or false positives.</p>"},{"location":"glossary/overall_performance_dimensions/#representation","title":"Representation","text":""},{"location":"glossary/overall_performance_dimensions/#selection-rate","title":"Selection Rate","text":"<p>Selection Rate (or Base Rate) means the fraction of data points in each class classified as 1. The formula for the Selection Rate is:</p> \\[ \\text{Selection Rate} = P(\\hat{Y} = 1) = \\frac{TP + FP}{TP + FP + TN + FN} \\]"},{"location":"glossary/overall_performance_dimensions/#stability","title":"Stability","text":""},{"location":"glossary/overall_performance_dimensions/#label-stability","title":"Label Stability","text":"<p>Label Stability<sup>3</sup> is defined as the normalized absolute difference between the number of times  a sample is classified as positive or negative:</p> \\[  U_{h, \\text{stability}}(x) = \\frac{1}{m} (\\sum_{j=1}^m \\mathbf{1}[h_{D_j}(x)&gt;=0.5] - \\sum_{i=1}^m \\mathbf{1}[h_{D_i}(x)&lt;0.5]) \\] <p>where \\(x\\) is an unseen test sample, and \\(h_{D_j}(x)\\) is the predicted probability of the positive class of the \\(j^{\\text{th}}\\) model in the ensemble that has \\(m\\) estimators.</p> <p>Thus, Label Stability is a metric used to evaluate the level of disagreement between estimators in the ensemble.  When the absolute difference is large, the label is more stable. On the other hand, if the difference is exactly zero,  the estimator is said to be \"highly unstable\", meaning that a test sample has an equal probability of being classified  as positive or negative by the ensemble.</p>"},{"location":"glossary/overall_performance_dimensions/#jitter","title":"Jitter","text":"<p>Jitter<sup>4</sup> is another measure of the disagreement between estimators in the ensemble, for each individual test example.  It reuses a notion of \\(\\textit{Churn}\\)<sup>5</sup> to define a \"\\(\\textit{pairwise jitter}\\)\":</p> \\[ J_{i, j}\\left(p_\\theta\\right)=\\text{Churn}_{i, j}\\left(p_\\theta\\right)=\\frac{\\left|p_{\\theta i}(x) \\neq p_{\\theta j}(x)\\right|_{x \\in X}}{|X|} \\] <p>where \\(x\\) is an unseen test sample, and \\(p_{\\theta i}(x)\\), \\(p_{\\theta j}(x)\\) are the prediction labels of the \\(i^{\\text{th}}\\) and \\(j^{\\text{th}}\\) estimator in the ensemble for \\(x\\), respectively.</p> <p>To obtain the overall measure of disagreement across all models in the ensemble, we need to calculate  the average of \\(\\textit{pairwise jitters}\\) for all model pairs. This broader definition is referred to as \\(\\textit{jitter}\\):</p> \\[ J\\left(p_\\theta\\right)=\\frac{\\sum_{\\forall i, j \\in N} J_{i, j}\\left(p_\\theta\\right)}{N \\cdot(N-1) \\cdot \\frac{1}{2}} \\text{, where } i&lt;j \\]"},{"location":"glossary/overall_performance_dimensions/#uncertainty","title":"Uncertainty","text":""},{"location":"glossary/overall_performance_dimensions/#epistemic-uncertainty","title":"Epistemic Uncertainty","text":"<p>Epistemic (model) uncertainty captures arbitrariness in outcomes due to uncertainty over the \"true\" model parameters. If we knew the \"best\" model-type or hypothesis class to model each of our estimators in the bootstrap (\\(h_D\\)s) with, then, for different training datasets \\(D\\), we would fit the exact same model, and thereby have zero predictive variance. Following Tahir et al.<sup>6</sup>, we define the epistemic uncertainty as a measure of predictive variance:</p> \\[ U_{h, \\text{epistemic}}(x) = \\text{Var}_j[h_{D_j}(x)] \\] <p>where  \\(h_{D_j}(x)\\) is the predicted probability of the positive class.</p>"},{"location":"glossary/overall_performance_dimensions/#aleatoric-uncertainty","title":"Aleatoric Uncertainty","text":"<p>The source of aleatoric uncertainty is the inherent stochasticity present in the data that, in general, cannot be reduced/mitigated.  An intuitive way to think about aleatoric uncertainty is through an information-theoretic perspective: if there simply is not enough  (target-specific) information in a given data point, then even the \"best\" model will not be able to make a confident prediction for it. Following Tahir et al.<sup>6</sup>, we define the aleatoric uncertainty as the expected entropy for the prediction:</p> \\[ U_{h, \\text{aleatoric}}(x) = \\mathbb{E}_j[h_{D_j}(x)log(h_{D_j}(x)) + (1-h_{D_j}(x))log(1-h_{D_j}(x))] \\] <p>where  \\(h_{D_j}(x)\\) and 1- \\(h_{D_j}(x)\\) are the predicted probabilities of the positive and negative class respectively.</p> <p>References</p> <ol> <li> <p>Accuracy Score. Metrics and scoring: quantifying the quality of predictions. Link \u21a9</p> </li> <li> <p>F1 score, scikit-learn documentation. Link \u21a9</p> </li> <li> <p>Michael C. Darling and David J. Stracuzzi. \u201cToward Uncertainty Quantification for Supervised Classification\u201d. In: 2018.\u00a0\u21a9</p> </li> <li> <p>Huiting Liu et al. \u201cModel Stability with Continuous Data Updates\u201d. In: arXiv preprint arXiv:2201.05692 (2022).\u00a0\u21a9</p> </li> <li> <p>Mahdi Milani Fard et al. \u201cLaunch and iterate: Reducing prediction churn\u201d. In: Advances in Neural Information Processing Systems 29 (2016).\u00a0\u21a9</p> </li> <li> <p>Anique Tahir, Lu Cheng, and Huan Liu. 2023. Fairness through Aleatoric Uncertainty. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management (CIKM \u201923). Association for Computing Machinery, New York, NY, USA, 2372\u20132381.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"introduction/virny_overview/","title":"Virny Overview","text":"<p>The software framework decouples the process of model profiling into several stages, including subgroup metric computation, disparity metric composition, and metric visualization. This separation empowers data scientists with greater control and flexibility in employing the library, both during model development and for post-deployment monitoring. The above figure demonstrates how the library constructs a pipeline for model analysis. Inputs to a user interface are shown in green, pipeline stages are shown in blue, and the output of each stage is shown in purple.</p> <p>See more details in our SIGMOD demo paper.</p>"},{"location":"introduction/virnyview_overview/","title":"Interactive VirnyView Tool","text":"<ul> <li>Hosted interactive app for three fair-ML benchmark datasets</li> <li>Demonstrative Jupyter notebooks for the Virny capabilities</li> </ul>"},{"location":"introduction/virnyview_overview/#application-overview","title":"Application Overview","text":"<p>VirnyView serves as a visualization component within the Virny model profiling library, empowering data scientists to engage in responsible model selection and to generate a nutritional label for their model. Users can declaratively define various configurations for model profiling, supplying a dataset and models for scrutiny to Virny. This library computes overall and disparity metrics, profiling model accuracy, stability, uncertainty, and fairness.  Subsequently, the tool utilizes the output from Virny to construct an interactive web application for metric analytics.  This application allows users to scrutinize dataset properties related to protected groups, compare models  across diverse performance dimensions, and generate a comprehensive nutritional label for the most optimal model.</p> <p> Figure 1. A sample UI view of the second screen in the interactive web app.</p> <p>The application comprises six screens, each featuring user input options and a corresponding visualization  based on the provided inputs. To facilitate analysis, users have the flexibility to interactively choose a specific combination of models, overall metrics, and disparity metrics across various model dimensions. This selection dynamically alters the visualization perspective. Refer to Figure 1 for a visual representation of the sample web interface showcasing these screens.</p>"},{"location":"introduction/virnyview_overview/#user-flow-description","title":"User Flow Description","text":"<p>Our tool is developed with a user-friendly flow design specifically tailored to guide users through a responsible model selection process. This streamlined flow reduces the complexity associated with numerous model types and pre-, in-, and post-processing techniques. It facilitates a comprehensive comparison of selected models based on various performance dimensions using intuitive visualizations  such as colors and tolerance. Additionally, it allows users to break down the performance of a specific model  concerning multiple protected groups and performance dimensions.</p> <p>The user flow comprises six steps, as illustrated in Figures 2 and 3, and can be outlined as follows.</p> <p> Figure 2. Steps 1-4 in the user flow for responsible model selection.</p> <p>Step 1: Analyze demographic composition of the dataset.</p> <p>The application is structured from a high-level overview to a detailed examination. The upper screens provide general insights into the dataset and models, while the lower screens delve into the performance of individual models, dissected by protected groups. Thus, prior to delving into metric analysis, it is crucial to establish a comprehensive understanding of the proportions and  base rates of the protected groups within the dataset. This information serves to elucidate potential disparities, for example,  such as significant variations in overall accuracy and stability among different racial groups. Additionally, it can indicate  whether the model has learned something by comparing its accuracy to the base rate in  the dataset.</p> <p>Step 2: Reduce the number of models to compare based on overall and disparity metric constraints.</p> <p>Creating an accurate, robust, and fair model requires thorough validation of various model types, pre-processing techniques, and fairness interventions. However, the complexity arises when attempting to directly compare all models on a single plot or visualize every possible combination of the models. As a result, in the second step, we allow the user to define overall and disparity metric constraints to effectively narrow down the selection of models that meet these criteria. This strategic reduction allows for a more detailed comparison of diverse metrics, focusing on a more manageable number of models in the subsequent steps.</p> <p>Steps 3-4: Compare the selected models that satisfy all constraints using overall and disparity metric heatmaps.</p> <p>In the third and fourth steps, we select a set of models that satisfy all constraints from the second step,  and compare their overall and disparity metrics side-by-side. To enhance clarity, we added a color scheme, where green signifies the most favorable model metric and red denotes the least favorable compared to other models.</p> <p>It's crucial to acknowledge that the color scheme accommodates variations in optimal values for different metrics.  For instance, a score of 1.0 is optimal for Disparate Impact, while 0.0 is optimal for Equalized Odds FNR. Furthermore,  users have the option to introduce a tolerance parameter to the comparison process. This means that if the discrepancy  between metrics of different models falls below the tolerance threshold, these models are grouped together. This proves beneficial in cases where minor differences, such as 0.001%, can be considered negligible in comparison to the differences of other model metrics. Steps 3 and 4 collectively provide users with a better understanding of model performance across diverse dimensions relative to other models.  Subsequently, users can choose one or two models for a detailed breakdown of performance across protected groups  and generate a comprehensive nutritional label in the later steps.</p> <p> Figure 3. Steps 5-6 in the user flow for responsible model selection.</p> <p>Step 5: Generate a nutritional label for the selected model.</p> <p>In the fifth step, users choose a particular model and a combination of overall and disparity metrics to generate a nutritional label,  segmented by multiple protected groups and performance dimensions. The nutritional label includes bar charts for  the overall and disparity metrics presented side-by-side that helps to find interesting insights between them.  This graphical representation proves particularly effective in identifying performance gaps among binary or intersectional groups.</p> <p>Moreover, this visualization can be added to a model card, contributing to a responsible reporting mechanism that transparently  communicates the model's performance across diverse dimensions.</p> <p>Step 6: Summarize the performance of a particular model across different dimensions and protected groups.</p> <p>Finally, in the sixth step, users select a particular model and specify a desirable range for overall and disparity metrics encompassing accuracy, stability, uncertainty, and selection rate dimensions. This allows for a clear indication of whether a specific model  meets these defined constraints. The matrix adopts a color-coded scheme, where green signifies compliance with the constraints,  and red signals non-compliance. This final colored matrix serves as a model performance summary and can be incorporated into a model card,  similarly to the visualization generated in the fifth step.</p>"},{"location":"introduction/welcome_to_virny/","title":"Welcome to Virny","text":""},{"location":"introduction/welcome_to_virny/#description","title":"\ud83d\udcdc Description","text":"<p>Virny is a Python library for in-depth profiling of model performance across overall and disparity dimensions. In addition to its metric computation capabilities, the library provides an interactive tool called VirnyView to streamline responsible model selection and generate nutritional labels for ML models.</p> <p>The Virny library was developed based on three fundamental principles:</p> <p>1) easy extensibility of model analysis capabilities;</p> <p>2) compatibility to user-defined/custom datasets and model types;</p> <p>3) simple composition of disparity metrics based on the context of use.</p> <p>Virny decouples model auditing into several stages, including: subgroup metric computation, disparity metric composition, and metric visualization. This gives data scientists more control and flexibility to use the library for model development and monitoring post-deployment.</p> <p>For quickstart, look at use case examples, an interactive demo, and a demonstrative Jupyter notebook.</p>"},{"location":"introduction/welcome_to_virny/#installation","title":"\ud83d\udee0 Installation","text":"<p>Virny supports Python 3.9-3.12 and can be installed with <code>pip</code>:</p> <pre><code>pip install virny\n</code></pre>"},{"location":"introduction/welcome_to_virny/#documentation","title":"\ud83d\udcd2 Documentation","text":"<ul> <li>Introduction</li> <li>API Reference</li> <li>Use Case Examples</li> <li>Interactive Demo</li> </ul>"},{"location":"introduction/welcome_to_virny/#why-virny","title":"\ud83d\ude0e Why Virny","text":"<p>In contrast to existing fairness software libraries and model card generating frameworks, our system stands out in four key aspects:</p> <ol> <li> <p>Virny facilitates the measurement of all normatively important performance dimensions (including fairness, stability, and uncertainty) for a set of initialized models, both overall and broken down by user-defined subgroups of interest.</p> </li> <li> <p>Virny enables data scientists to analyze performance using multiple sensitive attributes (including non-binary) and their intersections.</p> </li> <li> <p>Virny offers diverse APIs for metric computation, designed to analyze multiple models in a single execution, assessing stability and uncertainty on correct and incorrect predictions broken down by protected groups, and testing models on multiple test sets, including in-domain and out-of-domain.</p> </li> <li> <p>Virny implements streamlined flow design tailored for responsible model selection, reducing the complexity associated with numerous model types, performance dimensions, and data-centric and model-centric interventions.</p> </li> </ol>"},{"location":"introduction/welcome_to_virny/#list-of-features","title":"\ud83d\udca1 List of Features","text":"<ul> <li>Profiling of all normatively important performance dimensions: accuracy, stability, uncertainty, and fairness</li> <li>Ability to analyze non-binary sensitive attributes and their intersections</li> <li>Convenient metric computation interfaces: an interface for multiple models, an interface for multiple test sets, and an interface for saving results into a user-defined database</li> <li>Interactive VirnyView visualizer that profiles dataset properties related to protected groups, computes comprehensive nutritional labels for individual models, compares multiple models according to multiple metrics, and guides users through model selection</li> <li>Compatibility with pre-, in-, and post-processors for fairness enhancement from AIF360</li> <li>An <code>error_analysis</code> computation mode to analyze model stability and confidence for correct and incorrect prodictions broken down by groups</li> <li>Metric static and interactive visualizations</li> <li>Data loaders with subsampling for popular fair-ML benchmark datasets</li> <li>User-friendly parameters input via config yaml files</li> <li>Integration with PyTorch Tabular</li> </ul> <p>Check out our documentation for a comprehensive overview.</p>"},{"location":"introduction/welcome_to_virny/#affiliations","title":"\ud83e\udd17 Affiliations","text":""},{"location":"introduction/welcome_to_virny/#citation","title":"\ud83d\udcac Citation","text":"<p>If Virny has been useful to you, and you would like to cite it in a scientific publication, please refer to the paper published at SIGMOD:</p> <pre><code>@inproceedings{herasymuk2024responsible,\n  title={Responsible Model Selection with Virny and VirnyView},\n  author={Herasymuk, Denys and Arif Khan, Falaah and Stoyanovich, Julia},\n  booktitle={Companion of the 2024 International Conference on Management of Data},\n  pages={488--491},\n  year={2024}\n}\n</code></pre>"},{"location":"introduction/welcome_to_virny/#license","title":"\ud83d\udcdd License","text":"<p>Virny is free and open-source software licensed under the 3-clause BSD license.</p>"},{"location":"release_notes/0.1.0/","title":"0.1.0 - 2023-02-06","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.1.0/#models-audit-pipeline","title":"\ud83d\ude80 Models Audit Pipeline","text":"<ul> <li> <p>Developed an entire pipeline for auditing model stability and fairness with detailed reports and visualizations</p> </li> <li> <p>Designed and implemented an extensible architecture split on components (User interfaces, MetricsComposer, etc.) that can be easily adapted to your needs</p> </li> <li> <p>Enabled easy pipeline adaptability for different classification datasets</p> </li> <li> <p>Added a feature to audit blind classifiers, which were trained on features without sensitive attributes, and use these sensitive attributes for analysis</p> </li> </ul>"},{"location":"release_notes/0.1.0/#user-interfaces","title":"\ud83d\udc69\u200d\ud83d\udcbb User Interfaces","text":"<ul> <li> <p>Added three types of user interfaces:</p> <ul> <li>Interface for multiple runs and multiple models</li> <li>Interface for multiple models and one run</li> <li>Interface for one model and one run</li> </ul> </li> <li> <p>Added an ability to input arguments to interfaces via user-friendly config yaml files or direct arguments</p> </li> </ul>"},{"location":"release_notes/0.1.0/#datasets-and-preprocessing","title":"\ud83d\uddc3 Datasets and Preprocessing","text":"<ul> <li> <p>Added built-in preprocessing techniques for raw classification datasets</p> </li> <li> <p>Developed an ability to work with non-binary features</p> </li> <li> <p>Enabled access to COMPAS and Folktables benchmark datasets via implemented data loaders</p> </li> </ul>"},{"location":"release_notes/0.1.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>Added an ability to analyze intersections of sensitive attributes</p> </li> <li> <p>Implemented a set of error and variance metrics:</p> <ul> <li>6 subgroup variance metrics<ul> <li>Mean</li> <li>Std</li> <li>IQR</li> <li>Entropy</li> <li>Jitter</li> <li>Label Stability</li> </ul> </li> <li>8 subgroup error metrics<ul> <li>TPR</li> <li>TNR</li> <li>PPV</li> <li>FNR</li> <li>FPR</li> <li>Accuracy</li> <li>F1</li> <li>Selection-Rate</li> </ul> </li> <li>5 group variance metrics<ul> <li>Label Stability Ratio</li> <li>IQR Difference</li> <li>Std Difference</li> <li>Std Ratio</li> <li>Jitter Difference</li> </ul> </li> <li>5 group fairness metrics<ul> <li>Equalized Odds TPR</li> <li>Equalized Odds FPR</li> <li>Disparate Impact</li> <li>Statistical Parity Difference</li> <li>Accuracy Difference</li> </ul> </li> </ul> </li> </ul>"},{"location":"release_notes/0.1.0/#reports-and-visualizations","title":"\ud83d\udcc8 Reports and Visualizations","text":"<ul> <li> <p>Added an ability to create predefined plots for result metrics</p> </li> <li> <p>Developed a feature to make detailed summary reports with visualizations</p> </li> </ul>"},{"location":"release_notes/0.1.0/#convenience","title":"\ud83d\ude0c Convenience","text":"<ul> <li> <p>Enabled smart saving of result metrics in files</p> </li> <li> <p>In the multiple runs interface, a file with result metrics is saved each time when each run is completed. In such a way, if you get an error in one of the runs, the results of the previous runs will be saved.</p> </li> <li> <p>Enabled library installation via pip</p> </li> <li> <p>Created and hosted a website for detailed documentation with examples</p> </li> </ul>"},{"location":"release_notes/0.2.0/","title":"0.2.0 - 2023-05-15","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.2.0/#user-interfaces","title":"\ud83d\udc69\u200d\ud83d\udcbb User Interfaces","text":"<ul> <li>Added two new types of user interfaces:<ul> <li>Multiple runs, multiple models with DB writer. This interface has the same functionality as the previous one,    but result metrics are stored in a user database. For that, users need to pass a DB writer function to the interface that can write result   metrics to their database. After each metrics computation run, the interface will use this function to save results in the database.</li> <li>Multiple runs, multiple models with several test sets. Except for a traditional flow   with one test set, Virny has an interface to work with many test sets that could   be used for model stress testing. This interface uses the same estimators in the bootstrap   for inference on the input test sets and saves metrics for each test set in a user   database. In such a way, a model comparison on different test sets is faster and more accurate.</li> </ul> </li> </ul>"},{"location":"release_notes/0.2.0/#new-benchmark-fair-ml-datasets","title":"\ud83d\uddc3 New Benchmark Fair-ML Datasets","text":"<ul> <li>Added 5 new data loaders for all tasks in the folktables benchmark<ul> <li>ACSIncomeDataset. A data loader for the income task from the folktables dataset.   Target: binary classification, predict if a person has an annual income &gt; $50,000.</li> <li>ACSEmploymentDataset. A data loader for the employment task from the folktables dataset.   Target: binary classification, predict if a person is employed.</li> <li>ACSMobilityDataset. A data loader for the mobility task from the folktables dataset.   Target: binary classification, predict whether a young adult moved addresses in the last year.</li> <li>ACSPublicCoverageDataset. A data loader for the public coverage task from the folktables dataset.   Target: binary classification, predict whether a low-income individual, not eligible for Medicare,   has coverage from public health insurance.</li> <li>ACSTravelTimeDataset. A data loader for the travel time task from the folktables dataset.   Target: binary classification, predict whether a working adult has a travel time to work of greater than 20 minutes.</li> </ul> </li> </ul>"},{"location":"release_notes/0.2.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>Developed an ability to define subgroups based on a list of values, e.g., create a subgroup based on values from 30 to 45 for the age column.</p> </li> <li> <p>Extended the ability to define intersectional groups based on 3 or more columns and conditions.</p> </li> </ul>"},{"location":"release_notes/0.3.0/","title":"0.3.0 - 2023-08-14","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.3.0/#new-metrics-computation-mode","title":"\u2699\ufe0f New Metrics Computation Mode","text":"<ul> <li> <p>An <code>error_analysis</code> mode that measures subgroup and group metrics for correct and incorrect predictions, in addition to default groups.  For example, when a default computation mode measures metrics for sex_priv and sex_dis, an <code>error_analysis</code> mode measures metrics  for (sex_priv, sex_priv_correct, sex_priv_incorrect) and (sex_dis, sex_dis_correct, sex_dis_incorrect).  Therefore, a user can analyze how a model is stable or certain about its incorrect predictions.</p> </li> <li> <p>An example yaml file for the computation mode: <pre><code>dataset_name: COMPAS\nbootstrap_fraction: 0.8\nn_estimators: 50\ncomputation_mode: error_analysis\nsensitive_attributes_dct: {'sex': 1, 'race': 'African-American', 'sex&amp;race': None}\n</code></pre></p> </li> </ul>"},{"location":"release_notes/0.3.0/#new-benchmark-fair-ml-datasets","title":"\ud83d\uddc3 New Benchmark Fair-ML Datasets","text":"<ul> <li> <p>LawSchoolDataset. A data loader for the Law School dataset that contains sensitive attributes among feature columns. </p> <ul> <li>Target: binary classify whether a candidate would pass the bar exam or predict a student\u2019s first-year average grade (FYA).</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/law_school_clean.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf.</li> </ul> </li> <li> <p>RicciDataset. A data loader for the Ricci dataset that contains sensitive attributes among feature columns.</p> <ul> <li>Target: binary classify whether an individual obtains a promotion based on the exam results.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/ricci_race.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf.</li> </ul> </li> </ul>"},{"location":"release_notes/0.3.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>New subgroup metrics:</p> <ul> <li>Statistical Bias is a feature of a statistical technique or of its results whereby the expected value of the results differs from the true underlying quantitative parameter being estimated (ref).</li> <li>Aleatoric Uncertainty is a mean entropy of ensemble (ref).</li> <li>Overall Uncertainty is an entropy of mean prediction of ensemble (ref).</li> </ul> </li> <li> <p>Changed a reference group in a sensitive_attributes_dct: now a disadvantaged group is used as a reference to compute intersectional metrics. For example, if we need to compute metrics for sex &amp; race group (sex -- [male, female], race -- [white, black]), then sex&amp;race_dis would include records for black females, and sex&amp;race_priv would include all other records in a dataset.</p> </li> </ul>"},{"location":"release_notes/0.4.0/","title":"0.4.0 - 2024-01-29","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.4.0/#static-and-interactive-visualizations","title":"\ud83d\udcc8\ufe0f Static and Interactive Visualizations","text":"<ul> <li> <p>An interactive web app serves as a visualization component within the Virny model profiling library, empowering data scientists   to engage in responsible model selection and to generate nutritional labels for their models. This application allows users    to scrutinize dataset properties related to protected groups, compare models across diverse performance dimensions,   and generate a comprehensive nutritional label for the most optimal model. The demonstration of the web app is hosted on the Hugging Face space.   More details are in the Examples section of the documentation.</p> </li> <li> <p>Improved and extended static visualizations provided by the Metric Visualizer.</p> </li> </ul>"},{"location":"release_notes/0.4.0/#new-metric-computation-capabilities","title":"\u2699\ufe0f New Metric Computation Capabilities","text":"<ul> <li> <p>A new capability to input an inprocessor into a metric computation interface as a basic model to profile an in-processing fairness intervention.   Currently, only inprocessors from aif360 are supported.   More details are in the Examples section of the documentation.</p> </li> <li> <p>A new capability to input a postprocessor into a metric computation interface to use post-processing fairness interventions during model profiling.    Currently, only postprocessors from aif360 are supported.   More details are in the Examples section of the documentation.</p> </li> </ul>"},{"location":"release_notes/0.4.0/#analyzers-and-metrics","title":"\ud83d\udca0 Analyzers and Metrics","text":"<ul> <li> <p>Added a sample size for each protected group to an overall metrics matrix. Useful to know if the size of a protected group is big enough to be representative.</p> </li> <li> <p>Simplified adding new metrics. Now, all functions, which compute overall metrics, are defined in Virny's metrics package.</p> </li> <li> <p>Improved definition of disparity metrics. Now, all disparity metrics and their expressions are defined in the Metric Composer.</p> </li> </ul>"},{"location":"release_notes/0.4.0/#new-benchmark-fair-ml-dataset","title":"\ud83d\uddc3 New Benchmark Fair-ML Dataset","text":"<ul> <li>StudentPerformancePortugueseDataset. A data loader for the Student Performance dataset for the Portuguese subject that contains sensitive attributes among feature columns.<ul> <li>Target: The initial regression task was to predict the final year grade of the students. To get a binary classification task, we used a preprocessed dataset from this repo. The target label is derived from the attribute G3 (representing the final grade), where target = {Low, High}, corresponding to G3 = {&lt;10, \u226510}.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/student_por_new.csv.</li> <li>Broader description: https://arxiv.org/pdf/2110.00530.pdf (Section 3.4.1).</li> </ul> </li> </ul>"},{"location":"release_notes/0.5.0/","title":"0.5.0 - 2024-06-02","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.5.0/#sigmod-demo-paper","title":"\ud83d\udcdd SIGMOD Demo Paper","text":"<ul> <li>Virny demonstration paper was accepted and published at SIGMOD! \ud83c\udf89\ud83e\udd73 Explore our work in the ACM digital library.</li> </ul> <pre><code>@inproceedings{herasymuk2024responsible,\n  title={Responsible Model Selection with Virny and VirnyView},\n  author={Herasymuk, Denys and Arif Khan, Falaah and Stoyanovich, Julia},\n  booktitle={Companion of the 2024 International Conference on Management of Data},\n  pages={488--491},\n  year={2024}\n}\n</code></pre>"},{"location":"release_notes/0.5.0/#glossary","title":"\ud83d\udcd6 Glossary","text":"<ul> <li>Our documentation was extended with a new \"Glossary\" section that provides:<ul> <li>Explanation of our approach for measuring model stability and uncertainty</li> <li>Detailed description of the overall and disparity metrics computed by Virny</li> </ul> </li> </ul>"},{"location":"release_notes/0.5.0/#more-features-for-experimental-studies","title":"\u2699\ufe0f More Features for Experimental Studies","text":"<ul> <li> <p>Metric computation interfaces were extended with a new optional parameter - <code>with_predict_proba</code>.  If set to <code>False</code>, Virny computes metrics only based on prediction labels, NOT prediction probabilities. Specifically, it can be useful in the case when a model cannot provide probabilities for its predictions. Default: <code>True</code>.</p> </li> <li> <p>Metric computation interfaces were enabled to measure the computation runtime in minutes for each model.  It can be particularly useful for a large experimental studies with thousands of pipelines to estimate their runtimes and to benchmark models between each other.</p> </li> </ul>"},{"location":"release_notes/0.5.0/#new-benchmark-fair-ml-datasets","title":"\ud83d\uddc3 New Benchmark Fair-ML Datasets","text":"<ul> <li> <p>DiabetesDataset2019. A data loader for the Diabetes 2019 dataset that contains sensitive attributes among feature columns.</p> <ul> <li>Target: Binary classify whether a person has a diabetes disease or not.</li> <li>Source and broad description: https://www.kaggle.com/datasets/tigganeha4/diabetes-dataset-2019/data.</li> </ul> </li> <li> <p>GermanCreditDataset. A data loader for the German Credit dataset that contains sensitive attributes among feature columns.</p> <ul> <li>Target: Binary classify people described by a set of attributes as good or bad credit risks.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/german_data_credit.csv.</li> <li>General description and analysis: https://arxiv.org/pdf/2110.00530.pdf (Section 3.1.3).</li> <li>Broad description: https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data.</li> </ul> </li> <li> <p>BankMarketingDataset. A data loader for the Bank Marketing dataset that contains sensitive attributes among feature columns.</p> <ul> <li>Target: The data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit.</li> <li>Source: https://github.com/tailequy/fairness_dataset/blob/main/experiments/data/bank-full.csv.</li> <li>General description and analysis: https://arxiv.org/pdf/2110.00530.pdf (Section 3.1.5).</li> <li>Broad description: https://archive.ics.uci.edu/dataset/222/bank+marketing.</li> </ul> </li> <li> <p>CardiovascularDiseaseDataset. A data loader for the Cardiovascular Disease dataset that contains sensitive attributes among feature columns.</p> <ul> <li>Target: Binary classify whether a person has a cardiovascular disease or not.</li> <li>Source and broad description: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset.</li> </ul> </li> </ul>"},{"location":"release_notes/0.6.0/","title":"0.6.0 - 2024-09-02","text":"<ul> <li>PyPI</li> <li>GitHub</li> </ul>"},{"location":"release_notes/0.6.0/#new-python-versions-support","title":"\ud83d\ude80 New Python Versions Support","text":"<ul> <li>Now Virny supports Python 3.9, 3.10, 3.11, and 3.12! \ud83c\udf89\ud83e\udd73</li> </ul>"},{"location":"release_notes/0.6.0/#integration-with-pytorch-tabular","title":"\ud83d\udd25 Integration with PyTorch Tabular","text":"<ul> <li>Now Virny supports profiling of the tabular deep learning models from PyTorch Tabular</li> </ul>"},{"location":"release_notes/0.6.0/#fitted-bootstrap-exporting","title":"\u2699\ufe0f Fitted Bootstrap Exporting","text":"<ul> <li> <p>Added the <code>return_fitted_bootstrap</code> flag to metric computation interfaces to return a fitted bootstrap, which users can save to a pickle file later and reuse for future experiments</p> </li> <li> <p>Added the new <code>compute_metrics_with_fitted_bootstrap</code> interface in the inference API, where users can provide a fitted bootstrap, use it to do inference, and avoid the heavy bootstrap re-training to get metrics</p> </li> </ul>"}]}